{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7990f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\jatin\\appdata\\roaming\\python\\python39\\site-packages (0.5.0.20230113)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape) (4.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47add175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jatin\\anaconda3\\lib\\site-packages (1.5.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas --user --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea836ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fa126",
   "metadata": {},
   "source": [
    "# BJP TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d672a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44cc1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snscrape.modules.twitter as sntwitter\n",
    "#import pandas as pd\n",
    "#query=[\"(from:sambitswaraj) until:2014-05-12 since:2013-11-01\"]\n",
    "#tweets=[]\n",
    "#limit=1000000\n",
    "\n",
    "#for quer in query:\n",
    " #   for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    " #       if len(tweets)==limit:\n",
    "   #       break\n",
    "    #    else:\n",
    "     #       tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "#df=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "#print(df)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e954339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064478e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_14620\\3548712532.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([tweet.date,tweet.user.username,tweet.content])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Date       User  \\\n",
      "0     2014-05-11 14:36:55+00:00  BJP4India   \n",
      "1     2014-05-11 09:08:09+00:00  BJP4India   \n",
      "2     2014-05-11 04:16:29+00:00  BJP4India   \n",
      "3     2014-05-10 13:49:00+00:00  BJP4India   \n",
      "4     2014-05-10 10:47:00+00:00  BJP4India   \n",
      "...                         ...        ...   \n",
      "91788 2013-11-04 06:35:24+00:00  BJP4India   \n",
      "91789 2013-11-03 05:47:26+00:00  BJP4India   \n",
      "91790 2013-11-02 08:17:45+00:00  BJP4India   \n",
      "91791 2013-11-01 12:31:34+00:00  BJP4India   \n",
      "91792 2013-11-01 06:48:07+00:00  BJP4India   \n",
      "\n",
      "                                                   Tweet  \n",
      "0      Shri @AmitShahOffice demands proper security a...  \n",
      "1          एक नया वाराणसी बनाएं! https://t.co/NmMIWofmJe  \n",
      "2      Less than 24 hours left in last phase of polli...  \n",
      "3      एक वोट बहुमत दिलाएगा।\\n\\nअपने सभी मित्रों एवं ...  \n",
      "4      श्री नरेन्द्र मोदी मिशन २७२+ की अंतिम रैली बलि...  \n",
      "...                                                  ...  \n",
      "91788  Join Mission 272+ at http://t.co/b4gwV2qzPq. h...  \n",
      "91789  दीपावली की हार्दिक शुभकामनाएँ| http://t.co/5wh...  \n",
      "91790  Shri Gajendra Chauhan appeals people to get re...  \n",
      "91791  Syed @ShahnawazBJP spoke on the statement of K...  \n",
      "91792  Join Mission 272+ at http://t.co/b4gwV2qzPq. h...  \n",
      "\n",
      "[91793 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:BJP4India) until:2014-05-12 since:2013-11-01\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df0=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "print(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003e7abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-11 14:36:55+00:00</td>\n",
       "      <td>BJP4India</td>\n",
       "      <td>Shri @AmitShahOffice demands proper security a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-11 09:08:09+00:00</td>\n",
       "      <td>BJP4India</td>\n",
       "      <td>एक नया वाराणसी बनाएं! https://t.co/NmMIWofmJe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-11 04:16:29+00:00</td>\n",
       "      <td>BJP4India</td>\n",
       "      <td>Less than 24 hours left in last phase of polli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-10 13:49:00+00:00</td>\n",
       "      <td>BJP4India</td>\n",
       "      <td>एक वोट बहुमत दिलाएगा।\\n\\nअपने सभी मित्रों एवं ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-10 10:47:00+00:00</td>\n",
       "      <td>BJP4India</td>\n",
       "      <td>श्री नरेन्द्र मोदी मिशन २७२+ की अंतिम रैली बलि...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date       User  \\\n",
       "0 2014-05-11 14:36:55+00:00  BJP4India   \n",
       "1 2014-05-11 09:08:09+00:00  BJP4India   \n",
       "2 2014-05-11 04:16:29+00:00  BJP4India   \n",
       "3 2014-05-10 13:49:00+00:00  BJP4India   \n",
       "4 2014-05-10 10:47:00+00:00  BJP4India   \n",
       "\n",
       "                                               Tweet  \n",
       "0  Shri @AmitShahOffice demands proper security a...  \n",
       "1      एक नया वाराणसी बनाएं! https://t.co/NmMIWofmJe  \n",
       "2  Less than 24 hours left in last phase of polli...  \n",
       "3  एक वोट बहुमत दिलाएगा।\\n\\nअपने सभी मित्रों एवं ...  \n",
       "4  श्री नरेन्द्र मोदी मिशन २७२+ की अंतिम रैली बलि...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc8999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd0034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd9b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "BJP1_csv=df0.to_csv('BJP1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141e1d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_13988\\1262610598.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([tweet.date,tweet.user.username,tweet.content])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Date          User  \\\n",
      "0    2014-05-12 12:55:25+00:00  narendramodi   \n",
      "1    2014-05-12 12:54:39+00:00  narendramodi   \n",
      "2    2014-05-12 12:54:19+00:00  narendramodi   \n",
      "3    2014-05-12 12:53:51+00:00  narendramodi   \n",
      "4    2014-05-12 12:53:13+00:00  narendramodi   \n",
      "...                        ...           ...   \n",
      "9668 2013-11-01 09:47:25+00:00  sambitswaraj   \n",
      "9669 2013-11-01 08:03:13+00:00  sambitswaraj   \n",
      "9670 2013-11-01 07:56:29+00:00  sambitswaraj   \n",
      "9671 2013-11-01 07:18:41+00:00  sambitswaraj   \n",
      "9672 2013-11-01 03:12:06+00:00  sambitswaraj   \n",
      "\n",
      "                                                  Tweet  \n",
      "0     Today once again India has won! The power of t...  \n",
      "1     Due to social media, lies &amp; false promises...  \n",
      "2     Wherever I went it was a delight to interact w...  \n",
      "3     NDA remained firmly focussed on the agenda of ...  \n",
      "4     Usually ruling Party sets agenda of the electi...  \n",
      "...                                                 ...  \n",
      "9668  First time:3rd front parties meet-do not speak...  \n",
      "9669  Kapil Sibal dares Modi to debate, hmm what do ...  \n",
      "9670  Sibal blames Modi for Onion Price rise,Sheila ...  \n",
      "9671  Kapil Sibal wants somebody to teach History to...  \n",
      "9672  Great morning to all; festive odour in air; Ha...  \n",
      "\n",
      "[9673 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:narendramodi) until:2014-05-13 since:2013-11-01\",\"(from:arunjaitley) until:2014-05-12 since:2013-11-01\",\"(from:ChouhanShivraj) until:2014-05-12 since:2013-11-01\",\"(from:thekiranbedi) until:2014-05-12 since:2013-11-01\",\"(from:AmitShah) until:2014-05-12 since:2013-11-01\",\"(from:SushmaSwaraj) until:2014-05-12 since:2013-11-01\",\"(from:nitin_gadkari) until:2014-05-12 since:2013-11-01\",\"(from:sambitswaraj) until:2014-05-12 since:2013-11-01\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fb0154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-12 12:55:25+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Today once again India has won! The power of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-12 12:54:39+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Due to social media, lies &amp;amp; false promises...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-12 12:54:19+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Wherever I went it was a delight to interact w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-12 12:53:51+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>NDA remained firmly focussed on the agenda of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-12 12:53:13+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Usually ruling Party sets agenda of the electi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date          User  \\\n",
       "0 2014-05-12 12:55:25+00:00  narendramodi   \n",
       "1 2014-05-12 12:54:39+00:00  narendramodi   \n",
       "2 2014-05-12 12:54:19+00:00  narendramodi   \n",
       "3 2014-05-12 12:53:51+00:00  narendramodi   \n",
       "4 2014-05-12 12:53:13+00:00  narendramodi   \n",
       "\n",
       "                                               Tweet  \n",
       "0  Today once again India has won! The power of t...  \n",
       "1  Due to social media, lies &amp; false promises...  \n",
       "2  Wherever I went it was a delight to interact w...  \n",
       "3  NDA remained firmly focussed on the agenda of ...  \n",
       "4  Usually ruling Party sets agenda of the electi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb19da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BJP_csv=df.to_csv('BJP.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ac6e3",
   "metadata": {},
   "source": [
    "# Congress tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258dd635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae600f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_13988\\498707853.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3AINCIndia%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAgJGEuK-xzwsWgoCQpNHM1fUMEnEVkM11FYCJehgHREVGQVVMVDUBFewCFQAA&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\"))\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3AINCIndia%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAgJGEuK-xzwsWgoCQpNHM1fUMEnEVkM11FYCJehgHREVGQVVMVDUBFewCFQAA&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\")), ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\")), ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\")), ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\"))\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3AINCIndia%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAgJGEuK-xzwsWgoCQpNHM1fUMEnEVkM11FYCJehgHREVGQVVMVDUBFewCFQAA&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m quer \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(quer)\u001b[38;5;241m.\u001b[39mget_items():\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tweets)\u001b[38;5;241m==\u001b[39mlimit:\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:1546\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1543\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1544\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m paginationParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mV2, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor):\n\u001b[0;32m   1547\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_timeline_instructions_to_tweets(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:759\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    758\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 759\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    762\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:725\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    724\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[1;32m--> 725\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\base.py:225\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 225\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\base.py:221\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    219\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    220\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3AINCIndia%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAgJGEuK-xzwsWgoCQpNHM1fUMEnEVkM11FYCJehgHREVGQVVMVDUBFewCFQAA&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:INCIndia) until:2014-05-12 since:2013-11-01\",\"(from:ShashiTharoor) until:2014-05-12 since:2013-11-01\",\"(from:ghulamnazad) until:2014-05-12\",\"(from:KapilSibal) until:2014-05-12\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df2=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235965f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_csv=df2.to_csv('congress.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207dca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5878f03",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6b15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c46bb35",
   "metadata": {},
   "source": [
    "### Aam aadmi party\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea98704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:ArvindKejriwal) until:2014-05-12 since:2013-11-01\",\"(from:msisodia) until:2014-05-12 since:2013-11-01\",\"(from:AamAadmiParty) until:2014-05-12 since:2013-11-01\",\"(from:DrKumarVishwas) until:2014-05-01 since:2013-11-12\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df3=pd.DataFrame(tweets,columns=['Date','User','Tweet']) # creating a dataframe\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9af314",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4441af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aap_csv=df3.to_csv('aap.csv',index=None) # creating dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875c378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f94b9208",
   "metadata": {},
   "source": [
    "### Trinamool congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b54875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:AITCofficial) until:2014-05-12 since:2013-11-01\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df4=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7deb3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmc_csv=df4.to_csv('tmc.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6de3e",
   "metadata": {},
   "source": [
    "## Communist party of india(marxisist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04de5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:cpimspeak) until:2014-05-12\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df5=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63031e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "communist_csv=df5.to_csv('communist.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa7d425",
   "metadata": {},
   "source": [
    "## Babhujan Samaj party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c416b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fdef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\jatin\\anaconda3\\lib\\site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-23.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af0ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\jatin\\appdata\\local\\temp\\pip-req-build-0pulirvm\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 786815dd05681e2421cd03aa9acf5ab5c773bce9\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (3.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: lxml in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (4.8.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape==0.6.2.20230321.dev13+g786815d) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (1.7.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (PEP 517): started\n",
      "  Building wheel for snscrape (PEP 517): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.6.2.20230321.dev13+g786815d-py3-none-any.whl size=73847 sha256=29cdcb2de027b38802e7e660c78cc2de8e2c7c464f68454aec945d203199934b\n",
      "  Stored in directory: C:\\Users\\jatin\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-crnql2zh\\wheels\\1a\\ba\\e2\\39fa3a11802c4a622f2efc8be3f5ff854481051d0b4c95c1fd\n",
      "Successfully built snscrape\n",
      "Installing collected packages: snscrape\n",
      "  Attempting uninstall: snscrape\n",
      "    Found existing installation: snscrape 0.5.0.20230113\n",
      "    Uninstalling snscrape-0.5.0.20230113:\n",
      "      Successfully uninstalled snscrape-0.5.0.20230113\n",
      "Successfully installed snscrape-0.6.2.20230321.dev13+g786815d\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\jatin\\AppData\\Local\\Temp\\pip-req-build-0pulirvm'\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77cfbd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3Ayadavakhilesh%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3Ayadavakhilesh%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3Ayadavakhilesh%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m quer \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(quer)\u001b[38;5;241m.\u001b[39mget_items():\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tweets)\u001b[38;5;241m==\u001b[39mlimit:\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:1546\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1543\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1544\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m paginationParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mV2, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor):\n\u001b[0;32m   1547\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_timeline_instructions_to_tweets(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:759\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    758\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 759\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    762\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:725\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    724\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[1;32m--> 725\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\base.py:225\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 225\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\base.py:221\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    219\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    220\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%28from%3Ayadavakhilesh%29+until%3A2014-05-12+since%3A2013-11-01&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "query=[\"(from:yadavakhilesh) until:2014-05-12 since:2013-11-01\",\"(from:shivpalsinghyad) until:2014-05-12 since:2013-11-01\"]\n",
    "tweets=[]\n",
    "limit=1000000\n",
    "\n",
    "for quer in query:\n",
    "    for tweet in sntwitter.TwitterSearchScraper(quer).get_items():\n",
    "        if len(tweets)==limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content])\n",
    "df6=pd.DataFrame(tweets,columns=['Date','User','Tweet'])\n",
    "print(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6f1a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-11 13:01:35+00:00</td>\n",
       "      <td>yadavakhilesh</td>\n",
       "      <td>I have realized in this election that it is ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-10 16:06:22+00:00</td>\n",
       "      <td>yadavakhilesh</td>\n",
       "      <td>Thanks to all the people who joined me for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-09 04:05:05+00:00</td>\n",
       "      <td>yadavakhilesh</td>\n",
       "      <td>Last few hours of campaigning. http://t.co/Szp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-08 03:26:49+00:00</td>\n",
       "      <td>yadavakhilesh</td>\n",
       "      <td>At Bhadohi.\\n\\nThe Bhadohi district is the big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-04 13:45:13+00:00</td>\n",
       "      <td>yadavakhilesh</td>\n",
       "      <td>Please watch my interview tonight at 9pm, on N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date           User  \\\n",
       "0 2014-05-11 13:01:35+00:00  yadavakhilesh   \n",
       "1 2014-05-10 16:06:22+00:00  yadavakhilesh   \n",
       "2 2014-05-09 04:05:05+00:00  yadavakhilesh   \n",
       "3 2014-05-08 03:26:49+00:00  yadavakhilesh   \n",
       "4 2014-05-04 13:45:13+00:00  yadavakhilesh   \n",
       "\n",
       "                                               Tweet  \n",
       "0  I have realized in this election that it is ea...  \n",
       "1  Thanks to all the people who joined me for the...  \n",
       "2  Last few hours of campaigning. http://t.co/Szp...  \n",
       "3  At Bhadohi.\\n\\nThe Bhadohi district is the big...  \n",
       "4  Please watch my interview tonight at 9pm, on N...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9338fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bahujan_csv=df6.to_csv('bahujan.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608a185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca6a222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=its+the+elephant+since%3A2020-06-01+until%3A2020-07-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=its+the+elephant+since%3A2020-06-01+until%3A2020-07-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=its+the+elephant+since%3A2020-06-01+until%3A2020-07-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m tweets_list2 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Using TwitterSearchScraper to scrape data and append tweets to list\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mits the elephant since:2020-06-01 until:2020-07-31\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:1546\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1543\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1544\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m paginationParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mV2, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor):\n\u001b[0;32m   1547\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_timeline_instructions_to_tweets(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:759\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    758\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 759\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    762\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\modules\\twitter.py:725\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    724\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[1;32m--> 725\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\base.py:225\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 225\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\snscrape\\base.py:221\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    219\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    220\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=its+the+elephant+since%3A2020-06-01+until%3A2020-07-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('its the elephant since:2020-06-01 until:2020-07-31').get_items()):\n",
    "    if i>500:\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3637e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from GetOldTweets3) (4.8.0)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from GetOldTweets3) (1.4.3)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954bb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetOldTweets3 --querysearch \"europe refugees\" --maxtweets 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
