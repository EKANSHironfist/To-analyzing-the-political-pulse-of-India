{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dc853e",
   "metadata": {},
   "source": [
    "# Bjp Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534ef71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0229482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the CSV file into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbaf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BJP.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaf1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to preprocess the tweet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3755286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove leading/trailing white space\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the preprocessing function to the tweet text in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718cb1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['Tweet'].apply(lambda x: preprocess_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58466239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40775628",
   "metadata": {},
   "source": [
    "### tranlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02c66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: retrying in c:\\users\\jatin\\anaconda3\\lib\\site-packages (1.3.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: six>=1.7.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from retrying) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573b29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7674afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from retrying import retry\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@retry(wait_fixed=5000, stop_max_attempt_number=3)\n",
    "def translate_batch(batch):\n",
    "    try:\n",
    "        # Translate the Hindi text in the batch to English\n",
    "        batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while translating batch: {e}\")\n",
    "        raise\n",
    "\n",
    "# Define the translation function\n",
    "def translate(df, batch_size=100):\n",
    "    # Split the DataFrame into batches of size batch_size\n",
    "    batches = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    # Translate each batch separately\n",
    "    for i, batch in enumerate(batches):\n",
    "        logging.info(f\"Translating batch {i+1} of {len(batches)}\")\n",
    "        translate_batch(batch)\n",
    "        # Introduce a delay of 10 seconds between batches to avoid hitting the rate limits\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Combine the translated batches into a single DataFrame and return it\n",
    "    return pd.concat(batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0793212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Translating batch 1 of 97\n",
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_14444\\1059041364.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
      "INFO:root:Translating batch 2 of 97\n",
      "INFO:root:Translating batch 3 of 97\n",
      "INFO:root:Translating batch 4 of 97\n",
      "INFO:root:Translating batch 5 of 97\n",
      "INFO:root:Translating batch 6 of 97\n",
      "INFO:root:Translating batch 7 of 97\n",
      "INFO:root:Translating batch 8 of 97\n",
      "INFO:root:Translating batch 9 of 97\n",
      "INFO:root:Translating batch 10 of 97\n",
      "INFO:root:Translating batch 11 of 97\n",
      "INFO:root:Translating batch 12 of 97\n",
      "INFO:root:Translating batch 13 of 97\n",
      "INFO:root:Translating batch 14 of 97\n",
      "INFO:root:Translating batch 15 of 97\n",
      "INFO:root:Translating batch 16 of 97\n",
      "INFO:root:Translating batch 17 of 97\n",
      "INFO:root:Translating batch 18 of 97\n",
      "INFO:root:Translating batch 19 of 97\n",
      "INFO:root:Translating batch 20 of 97\n",
      "INFO:root:Translating batch 21 of 97\n",
      "INFO:root:Translating batch 22 of 97\n",
      "INFO:root:Translating batch 23 of 97\n",
      "INFO:root:Translating batch 24 of 97\n",
      "INFO:root:Translating batch 25 of 97\n",
      "INFO:root:Translating batch 26 of 97\n",
      "INFO:root:Translating batch 27 of 97\n",
      "INFO:root:Translating batch 28 of 97\n",
      "INFO:root:Translating batch 29 of 97\n",
      "INFO:root:Translating batch 30 of 97\n",
      "INFO:root:Translating batch 31 of 97\n",
      "INFO:root:Translating batch 32 of 97\n",
      "INFO:root:Translating batch 33 of 97\n",
      "INFO:root:Translating batch 34 of 97\n",
      "INFO:root:Translating batch 35 of 97\n",
      "INFO:root:Translating batch 36 of 97\n",
      "INFO:root:Translating batch 37 of 97\n",
      "INFO:root:Translating batch 38 of 97\n",
      "INFO:root:Translating batch 39 of 97\n",
      "INFO:root:Translating batch 40 of 97\n",
      "INFO:root:Translating batch 41 of 97\n",
      "INFO:root:Translating batch 42 of 97\n",
      "INFO:root:Translating batch 43 of 97\n",
      "INFO:root:Translating batch 44 of 97\n",
      "INFO:root:Translating batch 45 of 97\n",
      "INFO:root:Translating batch 46 of 97\n",
      "INFO:root:Translating batch 47 of 97\n",
      "INFO:root:Translating batch 48 of 97\n",
      "INFO:root:Translating batch 49 of 97\n",
      "INFO:root:Translating batch 50 of 97\n",
      "INFO:root:Translating batch 51 of 97\n",
      "INFO:root:Translating batch 52 of 97\n",
      "INFO:root:Translating batch 53 of 97\n",
      "INFO:root:Translating batch 54 of 97\n",
      "INFO:root:Translating batch 55 of 97\n",
      "INFO:root:Translating batch 56 of 97\n",
      "INFO:root:Translating batch 57 of 97\n",
      "INFO:root:Translating batch 58 of 97\n",
      "INFO:root:Translating batch 59 of 97\n",
      "INFO:root:Translating batch 60 of 97\n",
      "INFO:root:Translating batch 61 of 97\n",
      "INFO:root:Translating batch 62 of 97\n",
      "INFO:root:Translating batch 63 of 97\n",
      "INFO:root:Translating batch 64 of 97\n",
      "INFO:root:Translating batch 65 of 97\n",
      "INFO:root:Translating batch 66 of 97\n",
      "INFO:root:Translating batch 67 of 97\n",
      "INFO:root:Translating batch 68 of 97\n",
      "INFO:root:Translating batch 69 of 97\n",
      "INFO:root:Translating batch 70 of 97\n",
      "INFO:root:Translating batch 71 of 97\n",
      "INFO:root:Translating batch 72 of 97\n",
      "INFO:root:Translating batch 73 of 97\n",
      "INFO:root:Translating batch 74 of 97\n",
      "INFO:root:Translating batch 75 of 97\n",
      "INFO:root:Translating batch 76 of 97\n",
      "INFO:root:Translating batch 77 of 97\n",
      "INFO:root:Translating batch 78 of 97\n",
      "INFO:root:Translating batch 79 of 97\n",
      "INFO:root:Translating batch 80 of 97\n",
      "INFO:root:Translating batch 81 of 97\n",
      "INFO:root:Translating batch 82 of 97\n",
      "INFO:root:Translating batch 83 of 97\n",
      "INFO:root:Translating batch 84 of 97\n",
      "INFO:root:Translating batch 85 of 97\n",
      "INFO:root:Translating batch 86 of 97\n",
      "INFO:root:Translating batch 87 of 97\n",
      "INFO:root:Translating batch 88 of 97\n",
      "INFO:root:Translating batch 89 of 97\n",
      "INFO:root:Translating batch 90 of 97\n",
      "INFO:root:Translating batch 91 of 97\n",
      "INFO:root:Translating batch 92 of 97\n",
      "INFO:root:Translating batch 93 of 97\n",
      "INFO:root:Translating batch 94 of 97\n",
      "INFO:root:Translating batch 95 of 97\n",
      "INFO:root:Translating batch 96 of 97\n",
      "INFO:root:Translating batch 97 of 97\n"
     ]
    }
   ],
   "source": [
    "df = translate(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16eda87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sharmat Paray Sarvastav told that till May 16, he has kept a one-time bhajan, listening to the news of BJP's victory, his fast\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0da74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BJPsentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8acd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "585e4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-12 12:55:25+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Today once again India has won! The power of t...</td>\n",
       "      <td>today once again india has won the power of th...</td>\n",
       "      <td>today india power ballot amp spirit democracy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-12 12:54:39+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Due to social media, lies &amp;amp; false promises...</td>\n",
       "      <td>due to social media lies amp false promises of...</td>\n",
       "      <td>due social medium lie amp false promise severa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-12 12:54:19+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Wherever I went it was a delight to interact w...</td>\n",
       "      <td>wherever i went it was a delight to interact w...</td>\n",
       "      <td>wherever went delight interact local people so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-12 12:53:51+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>NDA remained firmly focussed on the agenda of ...</td>\n",
       "      <td>nda remained firmly focussed on the agenda of ...</td>\n",
       "      <td>nda remained firmly focussed agenda developmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-12 12:53:13+00:00</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Usually ruling Party sets agenda of the electi...</td>\n",
       "      <td>usually ruling party sets agenda of the electi...</td>\n",
       "      <td>usually ruling party set agenda election campa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date          User  \\\n",
       "0  2014-05-12 12:55:25+00:00  narendramodi   \n",
       "1  2014-05-12 12:54:39+00:00  narendramodi   \n",
       "2  2014-05-12 12:54:19+00:00  narendramodi   \n",
       "3  2014-05-12 12:53:51+00:00  narendramodi   \n",
       "4  2014-05-12 12:53:13+00:00  narendramodi   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  Today once again India has won! The power of t...   \n",
       "1  Due to social media, lies &amp; false promises...   \n",
       "2  Wherever I went it was a delight to interact w...   \n",
       "3  NDA remained firmly focussed on the agenda of ...   \n",
       "4  Usually ruling Party sets agenda of the electi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  today once again india has won the power of th...   \n",
       "1  due to social media lies amp false promises of...   \n",
       "2  wherever i went it was a delight to interact w...   \n",
       "3  nda remained firmly focussed on the agenda of ...   \n",
       "4  usually ruling party sets agenda of the electi...   \n",
       "\n",
       "                                       tokenize_text  \n",
       "0  today india power ballot amp spirit democracy ...  \n",
       "1  due social medium lie amp false promise severa...  \n",
       "2  wherever went delight interact local people so...  \n",
       "3  nda remained firmly focussed agenda developmen...  \n",
       "4  usually ruling party set agenda election campa...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"BJPsentiment.csv\")\n",
    "\n",
    "# Tokenization\n",
    "# df['text'] = df['text'].apply(lambda x: re.split('\\W+', x))\n",
    "# df.head()\n",
    "df['tokenize_text'] = df['text'].apply(lambda x: re.split('\\W+', str(x)) if type(x) == str else [])\n",
    "\n",
    "# Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokenize_text'] = df['tokenize_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Stemming # no stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokenize_text'] = df['tokenize_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "\n",
    "\n",
    "# Joining the tokenized words back into sentences\n",
    "df['tokenize_text'] = df['tokenize_text'].apply(lambda x: ' '.join(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85584d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9d68db",
   "metadata": {},
   "source": [
    "# 1 USING TEXTBOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c240a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b126888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to calculate the sentiment polarity of each tweet using TextBlob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c58c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1217d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the sentiment polarity function to the English tweets in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4210d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['tokenize_text'].apply(lambda x: get_sentiment_polarity(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00f372c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, visualize the sentiment distribution using a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0555a162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['polarity'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d638fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BJPsentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca9b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16106c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da845881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a3a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3391b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72822b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b9a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e1f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c9fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac402a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadb45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf1665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1d6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a0ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2da144a0",
   "metadata": {},
   "source": [
    "# Congress \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a24746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the CSV file into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9993ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('congress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to preprocess the tweet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a396783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove leading/trailing white space\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1442b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the preprocessing function to the tweet text in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcd0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'] = df1['Tweet'].apply(lambda x: preprocess_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8caa69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ed49fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: retrying in c:\\users\\jatin\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: six>=1.7.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from retrying) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a2ba2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99412065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from retrying import retry\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@retry(wait_fixed=5000, stop_max_attempt_number=3)\n",
    "def translate_batch(batch):\n",
    "    try:\n",
    "        # Translate the Hindi text in the batch to English\n",
    "        batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while translating batch: {e}\")\n",
    "        raise\n",
    "\n",
    "# Define the translation function\n",
    "def translate(df, batch_size=100):\n",
    "    # Split the DataFrame into batches of size batch_size\n",
    "    batches = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    # Translate each batch separately\n",
    "    for i, batch in enumerate(batches):\n",
    "        logging.info(f\"Translating batch {i+1} of {len(batches)}\")\n",
    "        translate_batch(batch)\n",
    "        # Introduce a delay of 10 seconds between batches to avoid hitting the rate limits\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Combine the translated batches into a single DataFrame and return it\n",
    "    return pd.concat(batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d26fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Translating batch 1 of 55\n",
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_7892\\1059041364.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
      "INFO:root:Translating batch 2 of 55\n",
      "INFO:root:Translating batch 3 of 55\n",
      "INFO:root:Translating batch 4 of 55\n",
      "INFO:root:Translating batch 5 of 55\n",
      "INFO:root:Translating batch 6 of 55\n",
      "INFO:root:Translating batch 7 of 55\n",
      "INFO:root:Translating batch 8 of 55\n",
      "INFO:root:Translating batch 9 of 55\n",
      "INFO:root:Translating batch 10 of 55\n",
      "INFO:root:Translating batch 11 of 55\n",
      "INFO:root:Translating batch 12 of 55\n",
      "INFO:root:Translating batch 13 of 55\n",
      "INFO:root:Translating batch 14 of 55\n",
      "INFO:root:Translating batch 15 of 55\n",
      "INFO:root:Translating batch 16 of 55\n",
      "INFO:root:Translating batch 17 of 55\n",
      "INFO:root:Translating batch 18 of 55\n",
      "INFO:root:Translating batch 19 of 55\n",
      "INFO:root:Translating batch 20 of 55\n",
      "INFO:root:Translating batch 21 of 55\n",
      "INFO:root:Translating batch 22 of 55\n",
      "INFO:root:Translating batch 23 of 55\n",
      "INFO:root:Translating batch 24 of 55\n",
      "INFO:root:Translating batch 25 of 55\n",
      "INFO:root:Translating batch 26 of 55\n",
      "INFO:root:Translating batch 27 of 55\n",
      "INFO:root:Translating batch 28 of 55\n",
      "INFO:root:Translating batch 29 of 55\n",
      "INFO:root:Translating batch 30 of 55\n",
      "INFO:root:Translating batch 31 of 55\n",
      "INFO:root:Translating batch 32 of 55\n",
      "INFO:root:Translating batch 33 of 55\n",
      "INFO:root:Translating batch 34 of 55\n",
      "INFO:root:Translating batch 35 of 55\n",
      "INFO:root:Translating batch 36 of 55\n",
      "INFO:root:Translating batch 37 of 55\n",
      "INFO:root:Translating batch 38 of 55\n",
      "INFO:root:Translating batch 39 of 55\n",
      "INFO:root:Translating batch 40 of 55\n",
      "INFO:root:Translating batch 41 of 55\n",
      "INFO:root:Translating batch 42 of 55\n",
      "INFO:root:Translating batch 43 of 55\n",
      "INFO:root:Translating batch 44 of 55\n",
      "INFO:root:Translating batch 45 of 55\n",
      "INFO:root:Translating batch 46 of 55\n",
      "INFO:root:Translating batch 47 of 55\n",
      "INFO:root:Translating batch 48 of 55\n",
      "INFO:root:Translating batch 49 of 55\n",
      "INFO:root:Translating batch 50 of 55\n",
      "INFO:root:Translating batch 51 of 55\n",
      "INFO:root:Translating batch 52 of 55\n",
      "INFO:root:Translating batch 53 of 55\n",
      "INFO:root:Translating batch 54 of 55\n",
      "INFO:root:Translating batch 55 of 55\n"
     ]
    }
   ],
   "source": [
    "df1 = translate(df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59a93f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You eat such a character\\nLevel your body and become a friend\\nGrave\\n\\nspeak without ego it will calm you & comfort others'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.text[4411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1298c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('congressTrans.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e96f3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-11 12:20:05+00:00</td>\n",
       "      <td>INCIndia</td>\n",
       "      <td>Watch: #RahulGandhi's speech in Chandauli yest...</td>\n",
       "      <td>watch  speech in chandauli yesterday</td>\n",
       "      <td>watch speech chandauli yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-11 12:15:38+00:00</td>\n",
       "      <td>INCIndia</td>\n",
       "      <td>Thank you Kashi! http://t.co/UURsXA61Z9</td>\n",
       "      <td>thank you kashi</td>\n",
       "      <td>thank kashi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-11 07:24:54+00:00</td>\n",
       "      <td>INCIndia</td>\n",
       "      <td>“@yuvadesh: Yuva Desh wishes all the mothers a...</td>\n",
       "      <td>yuva desh wishes all the mothers a very   rt</td>\n",
       "      <td>yuva desh wish mother rt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-11 04:39:09+00:00</td>\n",
       "      <td>INCIndia</td>\n",
       "      <td>Rahul Gandhi and @kashikirai greeting people d...</td>\n",
       "      <td>rahul gandhi and  greeting people during the j...</td>\n",
       "      <td>rahul gandhi greeting people jan sampark held ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-11 04:34:43+00:00</td>\n",
       "      <td>INCIndia</td>\n",
       "      <td>Ustad Bismillah Khan's family playing \"Raghupa...</td>\n",
       "      <td>Ustad Bismillah Khans Family Playing Raghupati...</td>\n",
       "      <td>Ustad Bismillah Khans Family Playing Raghupati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date      User  \\\n",
       "0  2014-05-11 12:20:05+00:00  INCIndia   \n",
       "1  2014-05-11 12:15:38+00:00  INCIndia   \n",
       "2  2014-05-11 07:24:54+00:00  INCIndia   \n",
       "3  2014-05-11 04:39:09+00:00  INCIndia   \n",
       "4  2014-05-11 04:34:43+00:00  INCIndia   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  Watch: #RahulGandhi's speech in Chandauli yest...   \n",
       "1            Thank you Kashi! http://t.co/UURsXA61Z9   \n",
       "2  “@yuvadesh: Yuva Desh wishes all the mothers a...   \n",
       "3  Rahul Gandhi and @kashikirai greeting people d...   \n",
       "4  Ustad Bismillah Khan's family playing \"Raghupa...   \n",
       "\n",
       "                                                text  \\\n",
       "0               watch  speech in chandauli yesterday   \n",
       "1                                    thank you kashi   \n",
       "2       yuva desh wishes all the mothers a very   rt   \n",
       "3  rahul gandhi and  greeting people during the j...   \n",
       "4  Ustad Bismillah Khans Family Playing Raghupati...   \n",
       "\n",
       "                                       tokenize_text  \n",
       "0                   watch speech chandauli yesterday  \n",
       "1                                        thank kashi  \n",
       "2                           yuva desh wish mother rt  \n",
       "3  rahul gandhi greeting people jan sampark held ...  \n",
       "4  Ustad Bismillah Khans Family Playing Raghupati...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load CSV file\n",
    "df1 = pd.read_csv(\"congressTrans.csv\")\n",
    "\n",
    "# Tokenization\n",
    "# df['text'] = df['text'].apply(lambda x: re.split('\\W+', x))\n",
    "# df.head()\n",
    "df1['tokenize_text'] = df1['text'].apply(lambda x: re.split('\\W+', str(x)) if type(x) == str else [])\n",
    "\n",
    "# Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df1['tokenize_text'] = df1['tokenize_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Stemming # no stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df1['tokenize_text'] = df1['tokenize_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "\n",
    "\n",
    "# Joining the tokenized words back into sentences\n",
    "df1['tokenize_text'] = df1['tokenize_text'].apply(lambda x: ' '.join(x))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97e119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a9a51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87ccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0b49d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['polarity'] = df1['tokenize_text'].apply(lambda x: get_sentiment_polarity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e00328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6851e259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df1['polarity'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5777480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('congressTrans.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfaee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the csv file containing tweets data\n",
    "df = pd.read_csv('tweets_data.csv')\n",
    "\n",
    "# create a new empty column 'sentiment' for storing the labeled sentiment\n",
    "df['sentiment'] = ''\n",
    "\n",
    "# loop through the first 150 rows of the data\n",
    "for i in range(150):\n",
    "    print('Tweet:', df.iloc[i]['text'])  # display the tweet text\n",
    "    sentiment = input('Enter sentiment (Positive, Negative or Neutral): ')  # prompt for sentiment label\n",
    "    df.at[i, 'sentiment'] = sentiment  # assign the labeled sentiment to the 'sentiment' column\n",
    "    \n",
    "# save the labeled data to a new csv file\n",
    "df.to_csv('labeled_tweets_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49256207",
   "metadata": {},
   "source": [
    "## APP party sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8a506fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594add24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a758d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('aap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b666ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a9c83ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove leading/trailing white space\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "de3bcad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['text'] = df2['Tweet'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364a31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b1ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "41284512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7dcf37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from retrying import retry\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@retry(wait_fixed=5000, stop_max_attempt_number=3)\n",
    "def translate_batch(batch):\n",
    "    try:\n",
    "        # Translate the Hindi text in the batch to English\n",
    "        batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while translating batch: {e}\")\n",
    "        raise\n",
    "\n",
    "# Define the translation function\n",
    "def translate(df, batch_size=100):\n",
    "    # Split the DataFrame into batches of size batch_size\n",
    "    batches = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    # Translate each batch separately\n",
    "    for i, batch in enumerate(batches):\n",
    "        logging.info(f\"Translating batch {i+1} of {len(batches)}\")\n",
    "        translate_batch(batch)\n",
    "        # Introduce a delay of 10 seconds between batches to avoid hitting the rate limits\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Combine the translated batches into a single DataFrame and return it\n",
    "    return pd.concat(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f0a67f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Translating batch 1 of 59\n",
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_8636\\898671073.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
      "INFO:root:Translating batch 2 of 59\n",
      "INFO:root:Translating batch 3 of 59\n",
      "INFO:root:Translating batch 4 of 59\n",
      "INFO:root:Translating batch 5 of 59\n",
      "INFO:root:Translating batch 6 of 59\n",
      "INFO:root:Translating batch 7 of 59\n",
      "INFO:root:Translating batch 8 of 59\n",
      "INFO:root:Translating batch 9 of 59\n",
      "INFO:root:Translating batch 10 of 59\n",
      "INFO:root:Translating batch 11 of 59\n",
      "INFO:root:Translating batch 12 of 59\n",
      "INFO:root:Translating batch 13 of 59\n",
      "INFO:root:Translating batch 14 of 59\n",
      "INFO:root:Translating batch 15 of 59\n",
      "INFO:root:Translating batch 16 of 59\n",
      "INFO:root:Translating batch 17 of 59\n",
      "INFO:root:Translating batch 18 of 59\n",
      "INFO:root:Translating batch 19 of 59\n",
      "INFO:root:Translating batch 20 of 59\n",
      "INFO:root:Translating batch 21 of 59\n",
      "INFO:root:Translating batch 22 of 59\n",
      "INFO:root:Translating batch 23 of 59\n",
      "INFO:root:Translating batch 24 of 59\n",
      "INFO:root:Translating batch 25 of 59\n",
      "INFO:root:Translating batch 26 of 59\n",
      "INFO:root:Translating batch 27 of 59\n",
      "INFO:root:Translating batch 28 of 59\n",
      "INFO:root:Translating batch 29 of 59\n",
      "INFO:root:Translating batch 30 of 59\n",
      "INFO:root:Translating batch 31 of 59\n",
      "INFO:root:Translating batch 32 of 59\n",
      "INFO:root:Translating batch 33 of 59\n",
      "INFO:root:Translating batch 34 of 59\n",
      "INFO:root:Translating batch 35 of 59\n",
      "INFO:root:Translating batch 36 of 59\n",
      "INFO:root:Translating batch 37 of 59\n",
      "INFO:root:Translating batch 38 of 59\n",
      "INFO:root:Translating batch 39 of 59\n",
      "INFO:root:Translating batch 40 of 59\n",
      "INFO:root:Translating batch 41 of 59\n",
      "INFO:root:Translating batch 42 of 59\n",
      "INFO:root:Translating batch 43 of 59\n",
      "INFO:root:Translating batch 44 of 59\n",
      "INFO:root:Translating batch 45 of 59\n",
      "INFO:root:Translating batch 46 of 59\n",
      "INFO:root:Translating batch 47 of 59\n",
      "INFO:root:Translating batch 48 of 59\n",
      "INFO:root:Translating batch 49 of 59\n",
      "INFO:root:Translating batch 50 of 59\n",
      "INFO:root:Translating batch 51 of 59\n",
      "INFO:root:Translating batch 52 of 59\n",
      "INFO:root:Translating batch 53 of 59\n",
      "INFO:root:Translating batch 54 of 59\n",
      "INFO:root:Translating batch 55 of 59\n",
      "INFO:root:Translating batch 56 of 59\n",
      "INFO:root:Translating batch 57 of 59\n",
      "INFO:root:Translating batch 58 of 59\n",
      "INFO:root:Translating batch 59 of 59\n"
     ]
    }
   ],
   "source": [
    "df2 = translate(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0bbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada0ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a935de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c246dba",
   "metadata": {},
   "source": [
    "# Hybrid aproach for sentiment analysis (AFINN and SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As SVM requires labeled data set ,and i have none ,so it is wise to fist label some rows of data.But As manual labeling is time consuming ,we will use ACTIVE Learnign for speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500782e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbda13ee",
   "metadata": {},
   "source": [
    "## Manual labeling before Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086dc261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: today india power ballot amp spirit democracy emerged supreme blog\n",
      "Enter sentiment (Positive, Negative or Neutral): positive\n",
      "Tweet: due social medium lie amp false promise several leader could go beyond podium rally power social medium\n",
      "Enter sentiment (Positive, Negative or Neutral): negative\n",
      "Tweet: wherever went delight interact local people social medium also helped understand sentiment\n",
      "Enter sentiment (Positive, Negative or Neutral): positive\n",
      "Tweet: nda remained firmly focussed agenda development amp good governance amp ensured became focal point entire campaign\n",
      "Enter sentiment (Positive, Negative or Neutral): positive\n",
      "Tweet: usually ruling party set agenda election campaign ruling party neither proactive responsive remained reactive\n",
      "Enter sentiment (Positive, Negative or Neutral): negative\n",
      "Tweet: biggest joy 2014 election increased turnout braving scorching heat amp rain people turned large number\n",
      "Enter sentiment (Positive, Negative or Neutral): positive\n",
      "Tweet: congratulate ec amp entire election amp security staff continuous effort entire election\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: india voted congratulation people india successful completion 2014 lok sabha election\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: also Shri Pa Sangma\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: curtain raiser 16th may state give massive mandate tdpbjp alliance parliament amp assembly election\n",
      "Enter sentiment (Positive, Negative or Neutral): positive\n",
      "Tweet: congratulation people seemandhra amp telangana giving favourable result today municipal election tdp amp bjp\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: voting today phase vote please go polling booth sharing video message\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: Met Chhattisgarh Come In Gandhinagar\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: special request youth go amp vote take family friend along polling booth\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: legal fraternity lost strong pillar shri tarakant jha also remembered passion towards maithili language\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative\n",
      "Tweet: shri tarakant jha lost dedicated leader played instrumental role building bjp may soul rest peace\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative\n",
      "Tweet: also sharing speech international conference defence offset last year vibrant gujarat summit\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: scientist strength help shape future knowledge era defence civilian impact\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: today think india become selfsufficient defence manufacturing strengthen rampd defence sector\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: ndas foreign policy also ensured nation opposed india testing gradually developed strong tie india various field\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: test 1998 victory technology amp nation willpower scientist worked hard develop nuclear programme\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: sharing last year blog contains portion atal ji historic speech 1998\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: atal ji india scripted new chapter history amp gave strong message world success test pokhran\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: greeting nation amp scientist community national technology day marking anniversary pokhran test 1998\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: something india soil make india special destined role jagat guru call let create strong amp developed india\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: overwhelmed people response assure repay affection unprecedented development\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: 8 month 5800 rally charchas programme 3 lakh kmsan extensive innovative amp satisfying journey blog\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: give bjp seat amp give u opportunity take amp india development journey newer height\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: received tremendous affection people entire campaign gratitude people\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: sp bsp amp congress cant good people 1 amp immersed corruption amp votebank politics\n",
      "Enter sentiment (Positive, Negative or Neutral): \n",
      "Tweet: another picture robertsganj\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: glimpse rally robertsganj earlier day\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: concluded campaign 2014 lok sabha election massive rally ballia thank people wonderful response\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: final day campaign address 5 rally across concluding rally ballia\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: At young age thinking fall Sharavasatva shook core writing like \n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: Sharmat Paray Sarvastav told till May 16 kept one time bhajan listening news BJP victory fast\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: Today I met late Parya Sharvastava daughter Sankash martyred Patna railway bombing Gapalgaj election rally\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: urged voting last phase set new record voting large number\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: today amp bihar one rally believe enthusiasm people looking nda ray hope\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: people sitting ac room say fatigued due duration poll mood ground suggests otherwise\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: despite congress effort bring caste amp communal politics campaign remained focussed agenda development amp good governance\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: day 1 congress attitude election nonserious talked everything apart core issue amp govt\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: thank everyone tirelessly worked success 3d rally rally would possible without effort\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: today final round bharat vijay 3d rally round addressed people 1300 location across india\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: today address final round bharat vijay 3d rally dont miss lifetime experience\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: siwan asked people want day return criminal given free hand answered\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: kerala minister came meet reprimanded cong leader wanted lata ji bharat ratna taken back politics untouchability\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: someone politics untouchability congress amp made political career following votebank politics\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: party president using term like unch amp neech suit madam fear defeat much say thing\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: mirzapurs brass carpet amp handloom industry face many problem amp brick kiln running govt amp centre done\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: addressed rally amp bihar picture mirzapur rally\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: rohaniya rally addressed 3 rally highlight\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: video shared rohaniya sabarmatis glory restored committed ganga\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: tourism giving better opportunity varanasis weaver lot done amp sure effort succeed\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: paid tribute shri sone lal patelhave fond memory interacting sure would proud statue unity initiative\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: rohaniya rally col nizamuddin joined u 100 year old amp devoted life nation joining azad hind fauj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: salute hardwork amp determination karyakartas varanasi selflessly devoted giving bjp record win varanasi\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: deeply grateful people varanasi affection\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: picture rohaniya rally\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: joined rally rohaniya amp visited party office varanasi city\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: amazing atmosphere rally rohaniya part varanasi lok sabha seat join\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: profound apology ganga maa able perform aarti today wish people know mother love politics\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: appeal karyakarta sister amp brother maintain peace amp ensure people face difficulty\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: unfortunate ec concerned institution neutrality amp karyakartas embark satyagraha\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: highlight campaign bihar today clear bihar going vote development amp stability\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: people bengal bless nda fully committed towards wellbeing west bengal amp youth\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: didi wanted bangladeshi infiltrator removed 2005 due votebank politics support people didnt expect poriborton\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: admired didi way fought left people bengal expected poriborton nothing changed last 3 yr didi\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: addressed rally kolkata barasat amp krishnanagar received phenomenal affection people bengal entire campaign\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: also call upon voting assembly poll ap amp voting assembly bypolls bihar hp amp wb vote record number\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: polling 64 seat across 7 state underway hope people vote large number amp youngster show way\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: nda firmly committed creating skilled india breaking away scam india upas misrule\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: pity mind cant see beyond balloon amp toffee today govt given call toffee yet another trophy\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: report also note simplicity process amp fact done minimum govt interference amp ensures good return farmer\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: report lauds gujs land acquisition amp allocation model best practice emulated state\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: wish friend congress keep abusing gujarat would read govts report lauded gujarat development\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: campaign 2014 election reaching final stage congress leader talked development issue sad\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: connected people voting final phase today 3d rally thanks people 100 location joined rally\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: address round 3d rally join\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: back campaign addressed 4 large rally highlight campaign\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: The height silver power hold place world rich powerful writer India \n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: The height lower ridge ridge last 60 year \n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: It may visible important role identification low caste height Tig Baldan Parsarith Dasha \n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: Understanding rip come lower class idol painted people \n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: delighted see superstar twitter warm welcome\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: best wish pilgrim india badrinath temple open today\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: vote vote progress amethi 60 month never yr\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: looted nation name poor son poor man challenging able digest\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: leader talking woman empowerment failed build facility educate girl constituency ironical\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: congress leader talk everything modi gujarat model spoken word done amethi\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: went amethi message change year 1 family ruined amethi went sow seed better future amethi\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: thank amethi send lotus delhi amethi amp make victorious record margin\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: amethi elected 3 member 1 family year get bjp charge sheet tell\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: addressing rally across address rally amethi support looking forward rally\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: amethyst untold story video released bjp press conference every indian watch voting\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: election process long people excitement increasing direction wind clear\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: 6 lakh weaver lost job package r 6000 cr announced nothing reached weaver deeply distressing\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: place addressed 3d rally significant number weaver appalling upa ignored wellbeing weaver\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: began speech expressing condolence family lost life train accident maharashtra prayer injured\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: people 100 location polling expected take place remaining 2 phase joined 3d rally today\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: happy door holy kedarnath temple opened pilgrim best wish pilgrim\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: address bharat vijay 3d rally looking forward connecting people country\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: support nda strong amp stable govt bring end votebank politics divided nation\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: short video capturing enthusiasm allahabad rally\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: sp amp bsp ruled year congress power delhi none party presented work record people shameful\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: instead improving people life cong leader go home poor photoops people punish insensitivity\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: sad hear complaint rigging amp violence part bengal amp ec ensure instance dont happen coming phase\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: didi may abuse committed wb progress believe positive change politics revenge\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: amazing rally asansol urged people elect babul supriyo youngster devoted serving people bengal\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: didi talk real amp paper tiger real tiger would never protected looted poor chit fund scam\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: congress left amp didi together upa fight wb concerned infiltrator bangladesh wb youth\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: Joined Rallies Bankura Asansol West Bengal Bhadohi Allahabad Up\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: congress icu may get many wellwishers jail wont add fortune\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: never forget patna rally since received lot affection bihar return affection record development\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: gangas condition worrisome contaminated water threat child want change politics humanity\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: shared idea comprehensive development poorvanchal neglected sp amp bsp lucknow amp upa delhi\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: bihar uttarakhand ap could see great enthusiasm among people today 3d rally thanks everyone joined\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: looking forward addressing bharat vijay 3d rally across country\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: nda usher fresh approach developing hill state people problem understood amp addressed effectively\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: talked step nda take improve life farmer terai region rally rudrapur\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: uttarakhand cm forgets union water resource minister equally responsible people suffering due flood last year\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: cm uttarakhand concerned dynastic politics people problem urged people roorkee reject dynastic politics\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: urged people uttarakhand send 5 lotus delhi amp reject congress misgovernance\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: rally almora\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: witnessed horror emergency freedom press amp freedom expression suppressed blot democracy\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: day feel sad see national tv channel struggling maintain professional freedom\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: greeting journalist friend world press freedom day free press democracy cornerstone amp must preserved letter amp spirit\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: location 3d rally andhra pradesh\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: looking forward 3d rally bihar location\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: apart 4 rally also address several bharat vijay 3d rally across uttarakhand location\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: campaign uttarakhand today rally srinagar almora rudrapur amp roorkee\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: colleague campaigned kargil amp dras thankful people overwhelming support bjp\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: tomorrow address 3d rally across ap bihar hp uttarakhand amp west bengal\n",
      "Enter sentiment (Positive, Negative or Neutral): Neutral\n",
      "Tweet: india one nda work hindu muslim realising dream people india\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: pm may spoken even 1100 time ever talk sonia ji nacnot planning commission mattered decision making\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: cong bothered power amp chair mother love understandable cost nation amp people aspiration\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: cong think survive repeating false promise mistaken people india called congs bluff amp punish\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: speaking could see large number people gathered various location derive lot strength enthusiasm\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: addressed 3d rally 100 place across india round addressed 900 3d rally last week\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: hope see 3d rally later evening location\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: dalit dast lucknow kisht ye asp bicep krish ki mitra body inis work\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: congress sp amp bsp fighting chair fighting happiness divide amp rule way unite amp progress\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: congress appetite corruption legendary earth sea till sky spared nothing accept longer\n",
      "Enter sentiment (Positive, Negative or Neutral): Negative \n",
      "Tweet: joined bharat vijay rally khalilabad basti gonda amp balrampur despite scorching heat rally witnessed record attendance\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: next 3 day address 3 round 3d rally location rally today\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: heartfelt greeting auspicious occasion parshuram jayanti may lord parshuram bless u courage amp knowledge\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: 2014 poll choice want scam india amp nda working hard create skilled india\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: people happy know endeavour build smart city across india amp commitment make seemandhra knowledge hub\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: youngster ap working gulf exploited upa bothered nda take step protect interest youngster\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n",
      "Tweet: upa integrated river seemandhras farmer would suffered fulfil atal ji dream river linking amp help farmer\n",
      "Enter sentiment (Positive, Negative or Neutral): Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the csv file containing tweets data\n",
    "df = pd.read_csv('BJPsentiment.csv')\n",
    "\n",
    "# create a new empty column 'sentiment' for storing the labeled sentiment\n",
    "df['sentiment'] = ''\n",
    "\n",
    "# loop through the first 150 rows of the data\n",
    "for i in range(150):\n",
    "    print('Tweet:', df.iloc[i]['tokenize_text'])  # display the tweet text\n",
    "    sentiment = input('Enter sentiment (Positive, Negative or Neutral): ')  # prompt for sentiment label\n",
    "    df.at[i, 'sentiment'] = sentiment  # assign the labeled sentiment to the 'sentiment' column\n",
    "    \n",
    "# save the labeled data to a new csv file\n",
    "df.to_csv('labeledBeforeALbjp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d67f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "##below code will divide the 'labeledBeforeALbjp.csv  into labeled_tweets_dataAL.csv'  and 'unlabeled_tweets_dataAL.csv' so that these csv file can be used as labeld and unlabeld dtaset for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef63e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv('labeledBeforeALbjp.csv')\n",
    "\n",
    "# Split the data into labeled and unlabeled data based on the sentiment column\n",
    "labeled_data = df[df['sentiment'].notnull()]\n",
    "unlabeled_data = df[df['sentiment'].isnull()]\n",
    "\n",
    "# Save the labeled and unlabeled data to separate CSV files\n",
    "labeled_data.to_csv('labeled_tweets_dataAL.csv', index=False)\n",
    "unlabeled_data.to_csv('unlabeled_tweets_dataAL.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161e84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86591307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW IS active learning with SVM using the labeled and unlabeled datasets you provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbd9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting modAL\n",
      "  Downloading modAL-0.4.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from modAL) (1.0.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from modAL) (1.5.3)\n",
      "Requirement already satisfied: scipy>=0.18 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from modAL) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from modAL) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pandas>=1.1.0->modAL) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pandas>=1.1.0->modAL) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.0->modAL) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->modAL) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->modAL) (1.1.0)\n",
      "Installing collected packages: modAL\n",
      "Successfully installed modAL-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install modAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232cbdaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM validation accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m query_idx, query_inst \u001b[38;5;241m=\u001b[39m al_svc\u001b[38;5;241m.\u001b[39mquery(tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(unlabeled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Label the queried instance by SVM\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m X_queried \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_inst\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m y_queried \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_queried)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Add the queried instance with its label to the labeled dataset\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2101\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2085\u001b[0m \n\u001b[0;32m   2086\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2099\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1379\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1381\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:687\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "\n",
    "# Load labeled dataset\n",
    "labeled_df = pd.read_csv('labeled_tweets_dataAL.csv')\n",
    "\n",
    "# Load unlabeled dataset\n",
    "unlabeled_df = pd.read_csv('unlabeled_tweets_dataAL.csv')\n",
    "\n",
    "# Define the function for feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Split labeled dataset into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(labeled_df['tokenize_text'], labeled_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the train and validation set\n",
    "X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_vec = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVC(kernel='linear',probability=True,random_state=42)\n",
    "\n",
    "# Train the SVM model on the labeled data\n",
    "svm.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the performance of the SVM model on the validation set\n",
    "svm_score = svm.score(X_val_vec, y_val)\n",
    "print(f'SVM validation accuracy: {svm_score}')\n",
    "\n",
    "# Initialize the active learner with SVM classifier and uncertainty sampling strategy\n",
    "al_svc = ActiveLearner(\n",
    "    estimator=svm,\n",
    "    X_training=X_train_vec,\n",
    "    y_training=y_train,\n",
    "    query_strategy=uncertainty_sampling\n",
    ")\n",
    "\n",
    "# Set the number of queries to be asked for labeling\n",
    "n_queries = 2000 - len(labeled_df)\n",
    "\n",
    "# Active learning loop\n",
    "for _ in range(n_queries):\n",
    "    # Query the unlabeled dataset for the instances that the active learner is most uncertain about\n",
    "    query_idx, query_inst = al_svc.query(tfidf_vectorizer.transform(unlabeled_df['tokenize_text'].fillna('').astype(str)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Label the queried instance by SVM\n",
    "    X_queried = tfidf_vectorizer.transform([query_inst])\n",
    "\n",
    "\n",
    "    y_queried = svm.predict(X_queried)\n",
    "\n",
    "    # Add the queried instance with its label to the labeled dataset\n",
    "    labeled_df = labeled_df.append({'tokenize_text': query_inst, 'sentiment': y_queried[0]}, ignore_index=True)\n",
    "\n",
    "    # Remove the queried instance from the unlabeled dataset\n",
    "    unlabeled_df.drop(query_idx, inplace=True)\n",
    "\n",
    "    # Re-vectorize the labeled dataset\n",
    "    X_train = labeled_df['tokenize_text']\n",
    "    y_train = labeled_df['sentiment']\n",
    "    X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Retrain the SVM model on the updated labeled dataset\n",
    "    svm.fit(X_train_vec, y_train)\n",
    "\n",
    "    # Update the active learner with the updated labeled dataset\n",
    "    al_svc.teach(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the performance of the SVM model on the updated labeled dataset\n",
    "X_labeled_vec = tfidf_vectorizer.transform(labeled_df['tokenize_text'])\n",
    "y_labeled = labeled_df['sentiment']\n",
    "svm_score = svm.score(X_labeled_vec, y_labeled)\n",
    "print(f'SVM updated labeled dataset accuracy: {svm_score}')\n",
    "\n",
    "# Use the trained SVM model to predict the sentiment of the remaining unlabeled data\n",
    "X_unlabeled_vec = tfidf_vectorizer.transform(unlabeled_df['tokenize_text'])\n",
    "y_unlabeled_pred = svm.predict(X_unlabeled_vec)\n",
    "\n",
    "# Add the predicted labels to the unlabeled dataset\n",
    "unlabeled_df['sentiment'] = y_unlabeled_pred\n",
    "\n",
    "# Combine the labeled and updated unlabeled datasets\n",
    "updated_labeled_df = labeled_df.append(unlabeled_df)\n",
    "\n",
    "## Export the updated labeled dataset as a CSV file\n",
    "updated_labeled_df.to_csv('updated_labeled_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e7119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25d0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8314e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM validation accuracy: 0.6333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_4360\\3201405542.py:66: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  labeled_df = labeled_df.append({'tokenize_text': query_inst, 'sentiment': y_queried[0]}, ignore_index=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m     X_unlabeled_vec \u001b[38;5;241m=\u001b[39m csr_matrix(X_unlabeled_vec)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Update the active learner with the updated labeled dataset\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     X_queried_vec \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_inst\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     al_svc\u001b[38;5;241m.\u001b[39mteach(X_queried_vec, [y_queried[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of the SVM model on the updated labeled dataset\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2101\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2085\u001b[0m \n\u001b[0;32m   2086\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2099\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1379\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1381\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:687\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load labeled dataset\n",
    "labeled_df = pd.read_csv('labeled_tweets_dataAL.csv')\n",
    "\n",
    "# Load unlabeled dataset\n",
    "unlabeled_df = pd.read_csv('unlabeled_tweets_dataAL.csv')\n",
    "\n",
    "# Concatenate labeled and unlabeled data\n",
    "concatenated_df = pd.concat([labeled_df, unlabeled_df], axis=0)\n",
    "# Fill NaN values with empty string\n",
    "concatenated_df['tokenize_text'] = concatenated_df['tokenize_text'].fillna('')\n",
    "\n",
    "# Define the function for feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize the concatenated data\n",
    "X_concatenated_vec = tfidf_vectorizer.fit_transform(concatenated_df['tokenize_text'])\n",
    "\n",
    "# Split the concatenated data back into labeled and unlabeled parts\n",
    "X_labeled_vec = X_concatenated_vec[:len(labeled_df)]\n",
    "X_unlabeled_vec = X_concatenated_vec[len(labeled_df):]\n",
    "\n",
    "# Split labeled dataset into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_labeled_vec, labeled_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Train the SVM model on the labeled data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the SVM model on the validation set\n",
    "svm_score = svm.score(X_val, y_val)\n",
    "print(f'SVM validation accuracy: {svm_score}')\n",
    "\n",
    "# Initialize the active learner with SVM classifier and uncertainty sampling strategy\n",
    "al_svc = ActiveLearner(\n",
    "    estimator=svm,\n",
    "    X_training=X_train,\n",
    "    y_training=y_train,\n",
    "    query_strategy=uncertainty_sampling\n",
    ")\n",
    "\n",
    "# Set the number of queries to be asked for labeling\n",
    "n_queries = 2000 - len(labeled_df)\n",
    "\n",
    "# Active learning loop\n",
    "for _ in range(n_queries):\n",
    "    # Query the unlabeled dataset for the instances that the active learner is most uncertain about\n",
    "    query_idx, query_inst = al_svc.query(X_unlabeled_vec)\n",
    "\n",
    "    # Label the queried instance by SVM\n",
    "    X_queried = X_unlabeled_vec[query_idx]\n",
    "    y_queried = svm.predict(X_queried).reshape(-1)\n",
    "\n",
    "\n",
    "    # Add the queried instance with its label to the labeled dataset\n",
    "    labeled_df = labeled_df.append({'tokenize_text': query_inst, 'sentiment': y_queried[0]}, ignore_index=True)\n",
    "\n",
    "    # Remove the queried instance from the unlabeled dataset\n",
    "    unlabeled_df = unlabeled_df.drop(query_idx)\n",
    "    X_unlabeled_vec = X_unlabeled_vec.toarray()\n",
    "    X_unlabeled_vec = np.delete(X_unlabeled_vec, query_idx, axis=0)\n",
    "    X_unlabeled_vec = csr_matrix(X_unlabeled_vec)\n",
    "\n",
    "    # Update the active learner with the updated labeled dataset\n",
    "    X_queried_vec = tfidf_vectorizer.transform([query_inst])\n",
    "    al_svc.teach(X_queried_vec, [y_queried[0]])\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the performance of the SVM model on the updated labeled dataset\n",
    "X_labeled_vec = tfidf_vectorizer.transform(labeled_df['tokenize_text'])\n",
    "y_labeled = labeled_df['sentiment']\n",
    "svm_score = svm.score(X_labeled_vec, y_labeled)\n",
    "print(f'SVM updated labeled dataset accuracy: {svm_score}')\n",
    "\n",
    "# Use the trained SVM model to predict the sentiment of the remaining unlabeled data\n",
    "X_unlabeled_vec = tfidf_vectorizer.transform(unlabeled_df['tokenize_text'])\n",
    "y_unlabeled_pred = svm.predict(X_unlabeled_vec)\n",
    "\n",
    "# Add the predicted labels to the unlabeled dataset\n",
    "unlabeled_df['sentiment'] = y_unlabeled_pred\n",
    "\n",
    "# Combine the labeled and updated unlabeled datasets\n",
    "updated_labeled_df = labeled_df.append(unlabeled_df)\n",
    "\n",
    "## Export the updated labeled dataset as a CSV file\n",
    "updated_labeled_df.to_csv('updated_labeled_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d3ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 127\n"
     ]
    }
   ],
   "source": [
    "nan_count = concatenated_df['tokenize_text'].isna().sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6261c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in the 'tokenize_text' column: 127\n"
     ]
    }
   ],
   "source": [
    "nan_count = concatenated_df['tokenize_text'].isna().sum()\n",
    "print(f\"Number of NaN values in the 'tokenize_text' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2279e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfaf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cdf03f5",
   "metadata": {},
   "source": [
    "## 2nd try but with pretrained dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84233a5",
   "metadata": {},
   "source": [
    "### preprocesss the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3df6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32f6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c777676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "907d629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove leading/trailing white space\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b6b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84180810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Translated'] = df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a44cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61d7c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb7453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "108cd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from retrying import retry\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@retry(wait_fixed=5000, stop_max_attempt_number=3)\n",
    "def translate_batch(batch):\n",
    "    try:\n",
    "        # Translate the Hindi text in the batch to English\n",
    "        batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while translating batch: {e}\")\n",
    "        raise\n",
    "\n",
    "# Define the translation function\n",
    "def translate(df, batch_size=100):\n",
    "    # Split the DataFrame into batches of size batch_size\n",
    "    batches = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    # Translate each batch separately\n",
    "    for i, batch in enumerate(batches):\n",
    "        logging.info(f\"Translating batch {i+1} of {len(batches)}\")\n",
    "        translate_batch(batch)\n",
    "        # Introduce a delay of 10 seconds between batches to avoid hitting the rate limits\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Combine the translated batches into a single DataFrame and return it\n",
    "    return pd.concat(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66befced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Translating batch 1 of 275\n",
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_3868\\898671073.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['text'] = batch['text'].apply(lambda x: translator.translate(x, dest='en').text)\n",
      "INFO:root:Translating batch 2 of 275\n",
      "INFO:root:Translating batch 3 of 275\n",
      "INFO:root:Translating batch 4 of 275\n",
      "INFO:root:Translating batch 5 of 275\n",
      "INFO:root:Translating batch 6 of 275\n",
      "INFO:root:Translating batch 7 of 275\n",
      "INFO:root:Translating batch 8 of 275\n",
      "INFO:root:Translating batch 9 of 275\n",
      "INFO:root:Translating batch 10 of 275\n",
      "INFO:root:Translating batch 11 of 275\n",
      "INFO:root:Translating batch 12 of 275\n",
      "INFO:root:Translating batch 13 of 275\n",
      "INFO:root:Translating batch 14 of 275\n",
      "INFO:root:Translating batch 15 of 275\n",
      "INFO:root:Translating batch 16 of 275\n",
      "INFO:root:Translating batch 17 of 275\n",
      "INFO:root:Translating batch 18 of 275\n",
      "INFO:root:Translating batch 19 of 275\n",
      "INFO:root:Translating batch 20 of 275\n",
      "INFO:root:Translating batch 21 of 275\n",
      "INFO:root:Translating batch 22 of 275\n",
      "INFO:root:Translating batch 23 of 275\n",
      "INFO:root:Translating batch 24 of 275\n",
      "INFO:root:Translating batch 25 of 275\n",
      "INFO:root:Translating batch 26 of 275\n",
      "INFO:root:Translating batch 27 of 275\n",
      "INFO:root:Translating batch 28 of 275\n",
      "INFO:root:Translating batch 29 of 275\n",
      "INFO:root:Translating batch 30 of 275\n",
      "INFO:root:Translating batch 31 of 275\n",
      "INFO:root:Translating batch 32 of 275\n",
      "INFO:root:Translating batch 33 of 275\n",
      "INFO:root:Translating batch 34 of 275\n",
      "INFO:root:Translating batch 35 of 275\n",
      "INFO:root:Translating batch 36 of 275\n",
      "INFO:root:Translating batch 37 of 275\n",
      "INFO:root:Translating batch 38 of 275\n",
      "INFO:root:Translating batch 39 of 275\n",
      "INFO:root:Translating batch 40 of 275\n",
      "INFO:root:Translating batch 41 of 275\n",
      "INFO:root:Translating batch 42 of 275\n",
      "INFO:root:Translating batch 43 of 275\n",
      "INFO:root:Translating batch 44 of 275\n",
      "INFO:root:Translating batch 45 of 275\n",
      "INFO:root:Translating batch 46 of 275\n",
      "INFO:root:Translating batch 47 of 275\n",
      "INFO:root:Translating batch 48 of 275\n",
      "INFO:root:Translating batch 49 of 275\n",
      "INFO:root:Translating batch 50 of 275\n",
      "INFO:root:Translating batch 51 of 275\n",
      "INFO:root:Translating batch 52 of 275\n",
      "INFO:root:Translating batch 53 of 275\n",
      "INFO:root:Translating batch 54 of 275\n",
      "INFO:root:Translating batch 55 of 275\n",
      "INFO:root:Translating batch 56 of 275\n",
      "INFO:root:Translating batch 57 of 275\n",
      "INFO:root:Translating batch 58 of 275\n",
      "INFO:root:Translating batch 59 of 275\n",
      "INFO:root:Translating batch 60 of 275\n",
      "INFO:root:Translating batch 61 of 275\n",
      "INFO:root:Translating batch 62 of 275\n",
      "INFO:root:Translating batch 63 of 275\n",
      "INFO:root:Translating batch 64 of 275\n",
      "INFO:root:Translating batch 65 of 275\n",
      "INFO:root:Translating batch 66 of 275\n",
      "INFO:root:Translating batch 67 of 275\n",
      "INFO:root:Translating batch 68 of 275\n",
      "INFO:root:Translating batch 69 of 275\n",
      "INFO:root:Translating batch 70 of 275\n",
      "INFO:root:Translating batch 71 of 275\n",
      "INFO:root:Translating batch 72 of 275\n",
      "INFO:root:Translating batch 73 of 275\n",
      "INFO:root:Translating batch 74 of 275\n",
      "INFO:root:Translating batch 75 of 275\n",
      "INFO:root:Translating batch 76 of 275\n",
      "INFO:root:Translating batch 77 of 275\n",
      "INFO:root:Translating batch 78 of 275\n",
      "INFO:root:Translating batch 79 of 275\n",
      "INFO:root:Translating batch 80 of 275\n",
      "INFO:root:Translating batch 81 of 275\n",
      "INFO:root:Translating batch 82 of 275\n",
      "INFO:root:Translating batch 83 of 275\n",
      "INFO:root:Translating batch 84 of 275\n",
      "INFO:root:Translating batch 85 of 275\n",
      "INFO:root:Translating batch 86 of 275\n",
      "INFO:root:Translating batch 87 of 275\n",
      "INFO:root:Translating batch 88 of 275\n",
      "INFO:root:Translating batch 89 of 275\n",
      "INFO:root:Translating batch 90 of 275\n",
      "INFO:root:Translating batch 91 of 275\n",
      "INFO:root:Translating batch 92 of 275\n",
      "INFO:root:Translating batch 93 of 275\n",
      "INFO:root:Translating batch 94 of 275\n",
      "INFO:root:Translating batch 95 of 275\n",
      "INFO:root:Translating batch 96 of 275\n",
      "INFO:root:Translating batch 97 of 275\n",
      "INFO:root:Translating batch 98 of 275\n",
      "INFO:root:Translating batch 99 of 275\n",
      "INFO:root:Translating batch 100 of 275\n",
      "INFO:root:Translating batch 101 of 275\n",
      "INFO:root:Translating batch 102 of 275\n",
      "INFO:root:Translating batch 103 of 275\n",
      "INFO:root:Translating batch 104 of 275\n",
      "INFO:root:Translating batch 105 of 275\n",
      "INFO:root:Translating batch 106 of 275\n",
      "INFO:root:Translating batch 107 of 275\n",
      "INFO:root:Translating batch 108 of 275\n",
      "INFO:root:Translating batch 109 of 275\n",
      "INFO:root:Translating batch 110 of 275\n",
      "INFO:root:Translating batch 111 of 275\n",
      "INFO:root:Translating batch 112 of 275\n",
      "INFO:root:Translating batch 113 of 275\n",
      "INFO:root:Translating batch 114 of 275\n",
      "INFO:root:Translating batch 115 of 275\n",
      "INFO:root:Translating batch 116 of 275\n",
      "INFO:root:Translating batch 117 of 275\n",
      "INFO:root:Translating batch 118 of 275\n",
      "INFO:root:Translating batch 119 of 275\n",
      "INFO:root:Translating batch 120 of 275\n",
      "INFO:root:Translating batch 121 of 275\n",
      "INFO:root:Translating batch 122 of 275\n",
      "INFO:root:Translating batch 123 of 275\n",
      "INFO:root:Translating batch 124 of 275\n",
      "INFO:root:Translating batch 125 of 275\n",
      "INFO:root:Translating batch 126 of 275\n",
      "INFO:root:Translating batch 127 of 275\n",
      "INFO:root:Translating batch 128 of 275\n",
      "INFO:root:Translating batch 129 of 275\n",
      "INFO:root:Translating batch 130 of 275\n",
      "INFO:root:Translating batch 131 of 275\n",
      "INFO:root:Translating batch 132 of 275\n",
      "INFO:root:Translating batch 133 of 275\n",
      "INFO:root:Translating batch 134 of 275\n",
      "INFO:root:Translating batch 135 of 275\n",
      "INFO:root:Translating batch 136 of 275\n",
      "INFO:root:Translating batch 137 of 275\n",
      "INFO:root:Translating batch 138 of 275\n",
      "INFO:root:Translating batch 139 of 275\n",
      "INFO:root:Translating batch 140 of 275\n",
      "INFO:root:Translating batch 141 of 275\n",
      "INFO:root:Translating batch 142 of 275\n",
      "INFO:root:Translating batch 143 of 275\n",
      "INFO:root:Translating batch 144 of 275\n",
      "INFO:root:Translating batch 145 of 275\n",
      "INFO:root:Translating batch 146 of 275\n",
      "INFO:root:Translating batch 147 of 275\n",
      "INFO:root:Translating batch 148 of 275\n",
      "INFO:root:Translating batch 149 of 275\n",
      "INFO:root:Translating batch 150 of 275\n",
      "INFO:root:Translating batch 151 of 275\n",
      "INFO:root:Translating batch 152 of 275\n",
      "INFO:root:Translating batch 153 of 275\n",
      "INFO:root:Translating batch 154 of 275\n",
      "INFO:root:Translating batch 155 of 275\n",
      "INFO:root:Translating batch 156 of 275\n",
      "INFO:root:Translating batch 157 of 275\n",
      "INFO:root:Translating batch 158 of 275\n",
      "INFO:root:Translating batch 159 of 275\n",
      "INFO:root:Translating batch 160 of 275\n",
      "INFO:root:Translating batch 161 of 275\n",
      "INFO:root:Translating batch 162 of 275\n",
      "INFO:root:Translating batch 163 of 275\n",
      "INFO:root:Translating batch 164 of 275\n",
      "INFO:root:Translating batch 165 of 275\n",
      "INFO:root:Translating batch 166 of 275\n",
      "INFO:root:Translating batch 167 of 275\n",
      "INFO:root:Translating batch 168 of 275\n",
      "INFO:root:Translating batch 169 of 275\n",
      "INFO:root:Translating batch 170 of 275\n",
      "INFO:root:Translating batch 171 of 275\n",
      "INFO:root:Translating batch 172 of 275\n",
      "INFO:root:Translating batch 173 of 275\n",
      "INFO:root:Translating batch 174 of 275\n",
      "INFO:root:Translating batch 175 of 275\n",
      "INFO:root:Translating batch 176 of 275\n",
      "INFO:root:Translating batch 177 of 275\n",
      "INFO:root:Translating batch 178 of 275\n",
      "INFO:root:Translating batch 179 of 275\n",
      "INFO:root:Translating batch 180 of 275\n",
      "INFO:root:Translating batch 181 of 275\n",
      "INFO:root:Translating batch 182 of 275\n",
      "INFO:root:Translating batch 183 of 275\n",
      "INFO:root:Translating batch 184 of 275\n",
      "INFO:root:Translating batch 185 of 275\n",
      "INFO:root:Translating batch 186 of 275\n",
      "INFO:root:Translating batch 187 of 275\n",
      "INFO:root:Translating batch 188 of 275\n",
      "INFO:root:Translating batch 189 of 275\n",
      "INFO:root:Translating batch 190 of 275\n",
      "INFO:root:Translating batch 191 of 275\n",
      "INFO:root:Translating batch 192 of 275\n",
      "INFO:root:Translating batch 193 of 275\n",
      "INFO:root:Translating batch 194 of 275\n",
      "INFO:root:Translating batch 195 of 275\n",
      "INFO:root:Translating batch 196 of 275\n",
      "INFO:root:Translating batch 197 of 275\n",
      "INFO:root:Translating batch 198 of 275\n",
      "INFO:root:Translating batch 199 of 275\n",
      "INFO:root:Translating batch 200 of 275\n",
      "INFO:root:Translating batch 201 of 275\n",
      "INFO:root:Translating batch 202 of 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Translating batch 203 of 275\n",
      "INFO:root:Translating batch 204 of 275\n",
      "INFO:root:Translating batch 205 of 275\n",
      "INFO:root:Translating batch 206 of 275\n",
      "INFO:root:Translating batch 207 of 275\n",
      "INFO:root:Translating batch 208 of 275\n",
      "INFO:root:Translating batch 209 of 275\n",
      "INFO:root:Translating batch 210 of 275\n",
      "INFO:root:Translating batch 211 of 275\n",
      "INFO:root:Translating batch 212 of 275\n",
      "INFO:root:Translating batch 213 of 275\n",
      "INFO:root:Translating batch 214 of 275\n",
      "INFO:root:Translating batch 215 of 275\n",
      "INFO:root:Translating batch 216 of 275\n",
      "INFO:root:Translating batch 217 of 275\n",
      "INFO:root:Translating batch 218 of 275\n",
      "INFO:root:Translating batch 219 of 275\n",
      "INFO:root:Translating batch 220 of 275\n",
      "INFO:root:Translating batch 221 of 275\n",
      "INFO:root:Translating batch 222 of 275\n",
      "INFO:root:Translating batch 223 of 275\n",
      "INFO:root:Translating batch 224 of 275\n",
      "INFO:root:Translating batch 225 of 275\n",
      "INFO:root:Translating batch 226 of 275\n",
      "INFO:root:Translating batch 227 of 275\n",
      "INFO:root:Translating batch 228 of 275\n",
      "INFO:root:Translating batch 229 of 275\n",
      "INFO:root:Translating batch 230 of 275\n",
      "INFO:root:Translating batch 231 of 275\n",
      "INFO:root:Translating batch 232 of 275\n",
      "INFO:root:Translating batch 233 of 275\n",
      "INFO:root:Translating batch 234 of 275\n",
      "INFO:root:Translating batch 235 of 275\n",
      "INFO:root:Translating batch 236 of 275\n",
      "INFO:root:Translating batch 237 of 275\n",
      "INFO:root:Translating batch 238 of 275\n",
      "INFO:root:Translating batch 239 of 275\n",
      "INFO:root:Translating batch 240 of 275\n",
      "INFO:root:Translating batch 241 of 275\n",
      "INFO:root:Translating batch 242 of 275\n",
      "INFO:root:Translating batch 243 of 275\n",
      "INFO:root:Translating batch 244 of 275\n",
      "INFO:root:Translating batch 245 of 275\n",
      "INFO:root:Translating batch 246 of 275\n",
      "INFO:root:Translating batch 247 of 275\n",
      "INFO:root:Translating batch 248 of 275\n",
      "INFO:root:Translating batch 249 of 275\n",
      "INFO:root:Translating batch 250 of 275\n",
      "INFO:root:Translating batch 251 of 275\n",
      "INFO:root:Translating batch 252 of 275\n",
      "INFO:root:Translating batch 253 of 275\n",
      "INFO:root:Translating batch 254 of 275\n",
      "INFO:root:Translating batch 255 of 275\n",
      "INFO:root:Translating batch 256 of 275\n",
      "INFO:root:Translating batch 257 of 275\n",
      "INFO:root:Translating batch 258 of 275\n",
      "INFO:root:Translating batch 259 of 275\n",
      "INFO:root:Translating batch 260 of 275\n",
      "INFO:root:Translating batch 261 of 275\n",
      "INFO:root:Translating batch 262 of 275\n",
      "INFO:root:Translating batch 263 of 275\n",
      "INFO:root:Translating batch 264 of 275\n",
      "INFO:root:Translating batch 265 of 275\n",
      "INFO:root:Translating batch 266 of 275\n",
      "INFO:root:Translating batch 267 of 275\n",
      "INFO:root:Translating batch 268 of 275\n",
      "INFO:root:Translating batch 269 of 275\n",
      "INFO:root:Translating batch 270 of 275\n",
      "INFO:root:Translating batch 271 of 275\n",
      "INFO:root:Translating batch 272 of 275\n",
      "INFO:root:Translating batch 273 of 275\n",
      "INFO:root:Translating batch 274 of 275\n",
      "INFO:root:Translating batch 275 of 275\n"
     ]
    }
   ],
   "source": [
    "df = translate(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f6005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ba468a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Tweetssentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88194c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a21bd989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Translated</th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>id responded going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>bos bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on th...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>sons of  why couldnt they put them on the rele...</td>\n",
       "      <td>son couldnt put release already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861  Sons of ****, why couldn`t they put them on th...   \n",
       "\n",
       "                         selected_text sentiment  \\\n",
       "0  I`d have responded, if I were going   neutral   \n",
       "1                             Sooo SAD  negative   \n",
       "2                          bullying me  negative   \n",
       "3                       leave me alone  negative   \n",
       "4                        Sons of ****,  negative   \n",
       "\n",
       "                                          Translated  \\\n",
       "0                  id have responded if i were going   \n",
       "1         sooo sad i will miss you here in san diego   \n",
       "2                             my boss is bullying me   \n",
       "3                      what interview leave me alone   \n",
       "4  sons of  why couldnt they put them on the rele...   \n",
       "\n",
       "                            tokenize_text  \n",
       "0                      id responded going  \n",
       "1                 sooo sad miss san diego  \n",
       "2                            bos bullying  \n",
       "3                   interview leave alone  \n",
       "4  son couldnt put release already bought  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"Tweetssentiment.csv\")\n",
    "\n",
    "# Tokenization\n",
    "# df['text'] = df['text'].apply(lambda x: re.split('\\W+', x))\n",
    "# df.head()\n",
    "df['tokenize_text'] = df['Translated'].apply(lambda x: re.split('\\W+', str(x)) if type(x) == str else [])\n",
    "\n",
    "# Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokenize_text'] = df['tokenize_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Stemming # no stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokenize_text'] = df['tokenize_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "\n",
    "\n",
    "# Joining the tokenized words back into sentences\n",
    "df['tokenize_text'] = df['tokenize_text'].apply(lambda x: ' '.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89dff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae7b1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Tweetssentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c38308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "507d4e79",
   "metadata": {},
   "source": [
    "### svm traing on prelabed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296c6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830d155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770dd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7aa3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e064140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the prelabeled sentiment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6b1d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is in a CSV file with columns: 'text' and 'sentiment'\n",
    "df = pd.read_csv('Tweetssentiment.csv')                 #USE \"TRANSLATED\" COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ad350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform any necessary preprocessing steps, such as removing noise, tokenization, lowercasing, etc.\n",
    "# You can use techniques like regular expressions, NLTK, or other text preprocessing libraries.\n",
    "# Ensure that your dataset has a clean and processed 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e97c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d4ae0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']  # Input features (text)\n",
    "y = df['sentiment']  # Target variable (sentiment labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccddfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bec396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf251546",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_test = X_test.dropna()\n",
    "X_train = X_train.dropna()\n",
    "y_train = y_train.dropna()\n",
    "\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c881f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cac2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SVM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be06a10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9684b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0391cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eb15da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10af46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6f697b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.709114062215754\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sving svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a88b478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming you have trained and obtained the 'svm_model' object\n",
    "\n",
    "# Save the trained model to a file\n",
    "filename = 'svm_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(svm_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b6da53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Load the model from the file\n",
    "# filename = 'svm_model.pkl'\n",
    "# with open(filename, 'rb') as file:\n",
    "#     pretrained_model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "04247b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'svm_model.sav'\n",
    "# pickle.dump(svm_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0c878",
   "metadata": {},
   "source": [
    "### fine tunning my model ,with my manual labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2ec0eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned SVM Accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.svm import SVC\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labeled_data = pd.read_csv('labeled_tweets_dataAL.csv')   \n",
    "\n",
    "# Step 1: Preprocess your data\n",
    "# Assuming you have a DataFrame called 'labeled_data' with columns 'tweet' and 'label'\n",
    "#preprocessed_data = preprocess_data(labeled_data['Tweet'])  # Apply necessary preprocessing steps\n",
    "\n",
    "# Step 2: Split your data into training and validation sets\n",
    "#X_train, X_val, y_train, y_val = train_test_split(preprocessed_data, labeled_data['sentiment'], test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(labeled_data['Tweet'], labeled_data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Load the pre-trained model\n",
    "#pretrained_model = SVC(kernel='linear')\n",
    "#pretrained_model.load_model('svm_model.pkl')\n",
    "#pretrained_model = joblib.load('svm_model.pkl')\n",
    "pretrained_model = pickle.load(open('svm_model.pkl', 'rb'))\n",
    "#pretrained_model = pickle.load(open('svm_model.sav', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "#: Vectorize the data\n",
    "tune_svm_vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = tune_svm_vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = tune_svm_vectorizer.transform(X_val)\n",
    "# Step 4: Replace or modify the final layers (if necessary)\n",
    "# Since you're using an SVM model, there are no final layers to modify\n",
    "\n",
    "# Step 5: Freeze some layers (optional)\n",
    "# Not applicable in the case of SVM\n",
    "\n",
    "# Step 6: Define the loss function and optimizer\n",
    "# Not applicable in the case of SVM\n",
    "\n",
    "# Step 7: Fine-tune the model\n",
    "tune_svm_model = pretrained_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Step 8: Evaluate the fine-tuned model\n",
    "accuracy = tune_svm_model.score(X_val_vectorized, y_val)\n",
    "print(\"Fine-tuned SVM Accuracy:\", accuracy)\n",
    "\n",
    "# Step 9: Adjust and iterate (if necessary)\n",
    "# Depending on the evaluation results, you can make further adjustments to the model or preprocessing steps if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e990b83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tune_svm_model_vectorizer.pkl']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Saving vectorizer\n",
    "joblib.dump(tune_svm_vectorizer, 'tune_svm_model_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d90b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1061dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f937326f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['Neutral']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed tweet\n",
    "preprocessed_tweet = \"Also sharing my speech at International Conference on Defence Offsets during last year's Vibrant Gujarat Summit http://t.co/0TfLvZRHEh\"\n",
    "\n",
    "# Vectorize the preprocessed tweet\n",
    "preprocessed_tweet_vectorized = tune_svm_vectorizer.transform([preprocessed_tweet])     ## using the tune_svm_vectorizer that was used in fine tune the svm model\n",
    "\n",
    "# Convert the vectorized tweet to a dense array\n",
    "preprocessed_tweet_vectorized = preprocessed_tweet_vectorized.toarray()\n",
    "\n",
    "# Remove extra dimension\n",
    "preprocessed_tweet_vectorized = preprocessed_tweet_vectorized.squeeze()\n",
    "\n",
    "# Predict the sentiment\n",
    "sentiment = tune_svm_model.predict([preprocessed_tweet_vectorized])\n",
    "\n",
    "# Print the predicted sentiment\n",
    "print(\"Predicted sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375265d",
   "metadata": {},
   "source": [
    "### tune svm model is saved as tune_svm_modelutf8.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1eccf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "import pickle\n",
    "\n",
    "# Assuming you have trained and obtained the 'svm_model' object\n",
    "\n",
    "# Save the trained model to a file\n",
    "filename = 'tune_svm_model.pkl'\n",
    "with open(filename, 'wb',) as file:\n",
    "    pickle.dump(tune_svm_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b84d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f367c5e",
   "metadata": {},
   "source": [
    "### hyperparameter tunning of above trained svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fc279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f13cf7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Accuracy:  0.4059423988333941\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'C': [0.1,1,10],\n",
    "#     'kernel': ['linear','rbf'],\n",
    "#     'gamma': [ 0.1,1,10]\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Perform grid search on the training data\n",
    "# grid_search.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# # Get the best hyperparameters and the corresponding model\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Evaluate the best model on the test data\n",
    "# accuracy = best_model.score(X_test_vectorized, y_test)\n",
    "\n",
    "# print(\"Best Hyperparameters: \", best_params)\n",
    "# print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fee960",
   "metadata": {},
   "source": [
    "# again trying to train svm but with afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae534eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f05e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from afinn import Afinn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807f3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prelabeled_data = pd.read_csv('Tweetssentiment.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "afinn_scores = np.array([afinn.score(tweet) for tweet in prelabeled_data['text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2030a631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in afinn_scores: 27481\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in afinn_scores:\", afinn_scores.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95effad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerPrelabeled = TfidfVectorizer()\n",
    "vectorized_tweets = vectorizerPrelabeled.fit_transform(prelabeled_data['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76b9d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in vectorized_tweets: 27481\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in vectorized_tweets:\", vectorized_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928d3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = hstack((vectorized_tweets, afinn_scores.reshape(-1, 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4213ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, prelabeled_data['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee18a1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_modelPrelabeled = SVC(kernel='linear')\n",
    "svm_modelPrelabeled.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "703523f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = svm_modelPrelabeled.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4193b722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731489903583773\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predicted_sentiments)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4701a",
   "metadata": {},
   "source": [
    "### tunnning the trained svm with afinn form my manuall labeld dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing all the stuff before split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8e1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e32095",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv('labeled_tweets_dataAL.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b24c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79467721",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labeled_data['Tweet']\n",
    "y = labeled_data['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "561c4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tweets = vectorizerPrelabeled.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce339a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of rows in vectorizer_tweets:\", vectorizer_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459e1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1da03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c8f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #afinn_scores = [afinn.score(tweet) for tweet in X]\n",
    "afinn_scores1 = np.array([afinn.score(tweet) for tweet in X ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3bd4619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in afinn_scores1: 149\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in afinn_scores1:\", afinn_scores1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "be3ff62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows_vectorizer_tweets = vectorizer_tweets.shape[0]\n",
    "# num_rows_afinn_scores1 = afinn_scores1.shape[0]\n",
    "\n",
    "# print(\"Number of rows in vectorizer_tweets:\", num_rows_vectorizer_tweets)\n",
    "# print(\"Number of rows in afinn_scores1:\", num_rows_afinn_scores1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c004386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Shape of vectorizer_tweets:\", vectorizer_tweets.shape)\n",
    "# print(\"Shape of afinn_scores1:\", afinn_scores1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "850860e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn_scores1 = afinn_scores1.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94162d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#afinn_scores = [afinn.score(tweet) for tweet in X]\n",
    "#afinn_scores = np.array(afinn_scores).reshape(-1, 1)  # Reshape afinn_scores\n",
    "combined_features = hstack((vectorizer_tweets, afinn_scores1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71064a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train1, X_test1, y_train1, y_test1 = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ed116ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_modelPrelabeled.fit(X_train1, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c9deb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = svm_modelPrelabeled.predict(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e38459ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# accuracy = (predicted_labels == y_test).mean()\n",
    "accuracy = accuracy_score(y_test1, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35ef63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa71e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in afinn_scores: 149\n",
      "Number of rows in vectorized_tweets: 27481\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in afinn_scores:\", afinn_scores.shape[0])\n",
    "print(\"Number of rows in vectorized_tweets:\", vectorized_tweets.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffa5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ae72e30",
   "metadata": {},
   "source": [
    "## HYPERPARAMETER TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb1a571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7323994906312534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from afinn import Afinn\n",
    "\n",
    "# Load the labeled dataset\n",
    "data = pd.read_csv('Tweetssentiment.csv')  # Replace 'labeled_data.csv' with your dataset file path\n",
    "\n",
    "# Separate the features and labels\n",
    "X = data['Tweet']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Initialize AFINN for feature extraction\n",
    "afinn = Afinn()\n",
    "\n",
    "# Extract features using AFINN\n",
    "X_afinn = np.array([afinn.score(tweet) for tweet in data['text']])\n",
    "X_afinn2 = X_afinn.reshape(-1, 1)\n",
    "\n",
    "# Convert text data into numerical feature vectors using TF-IDF\n",
    "hybrid_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = hybrid_vectorizer.fit_transform(X)\n",
    "\n",
    "# Concatenate AFINN features with TF-IDF features\n",
    "# Concatenate AFINN features with TF-IDF features\n",
    "X_combined = hstack((X_afinn2, X_tfidf))\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create the SVM classifier\n",
    "hybrid_svm = SVC()\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(hybrid_svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the SVM model with the best hyperparameters\n",
    "svm_best = SVC(**best_params)\n",
    "svm_best.fit(X_train2, y_train2)\n",
    "\n",
    "# Evaluate the model accuracy on the test set\n",
    "accuracy = svm_best.score(X_test2, y_test2)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f49ce",
   "metadata": {},
   "source": [
    "### Finetunning it with  150 rows of my  labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5c51805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 0.8666666666666667\n",
      "Validation/Test Accuracy: 0.8666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from afinn import Afinn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the prelabeled dataset from a CSV file\n",
    "df = pd.read_csv('labeled_tweets_dataAL.csv')\n",
    "\n",
    "# Separate the text and label columns\n",
    "texts = df['Tweet']#.tolist()\n",
    "labels = df['sentiment']#.tolist()\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "#vectorizer = TfidfVectorizer(stop_words='english')\n",
    "Xmanual1 = hybrid_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Create an instance of AFINN\n",
    "afinn_scores = Afinn()\n",
    "\n",
    "# Apply AFINN sentiment analysis to the texts\n",
    "afinn_scores1 = np.array([afinn_scores.score(text) for text in texts])\n",
    "afinn_scores2=afinn_scores1.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine AFINN scores with the TF-IDF matrix\n",
    "X_combined1 = hstack((Xmanual1, afinn_scores2))\n",
    "\n",
    "# Split the data into training and validation/test sets\n",
    "# train_texts = X_combined1[:120]\n",
    "# train_labels = labels[:120]\n",
    "# val_texts = X_combined1[120:]\n",
    "# val_labels = labels[120:]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_combined1, labels, test_size=0.1, random_state=44)\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "#svm_model = SVC(C=1.0, kernel='linear')\n",
    "svm_best.fit(X_train3, y_train3)\n",
    "\n",
    "predicted_labels1 = svm_best.predict(X_test3)\n",
    "\n",
    "# Evaluate the model on the validation/test set\n",
    "accuracy =  accuracy_score(predicted_labels1, y_test3)\n",
    "print(\"Validation\", accuracy)\n",
    "# Print the accuracy with full precision\n",
    "print(\"Validation/Test Accuracy: {:.10f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3eebc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_manual shape: (149, 1047)\n",
      "afinn_scores_sparse shape: (27481, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_manual shape:\", Xmanual1.shape)\n",
    "print(\"afinn_scores_sparse shape:\", afinn_scores2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac1b1c",
   "metadata": {},
   "source": [
    "## predicting on my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2096f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 20408 features, but SVC is expecting 1048 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m combined_databjp \u001b[38;5;241m=\u001b[39m hstack((vectorized_databjp, afinn_scoresbjp1))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Make predictions using the trained SVM model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m predictionsbjp \u001b[38;5;241m=\u001b[39m \u001b[43msvm_best\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_databjp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Save predictions to a CSV file or use them for further analysis\u001b[39;00m\n\u001b[0;32m     30\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictionsbjp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:791\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    789\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 791\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:414\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:592\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    589\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 592\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[0;32m    602\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 20408 features, but SVC is expecting 1048 features as input."
     ]
    }
   ],
   "source": [
    "# # Load the vectorizer\n",
    "# with open('vectorizer.pkl', 'rb') as f:\n",
    "#     vectorizer = pickle.load(f)\n",
    "\n",
    "# Load the prelabeled dataset from a CSV file\n",
    "data1 = pd.read_csv('BJP.csv')\n",
    "\n",
    "# Preprocess the new data\n",
    "#preprocessed_data = preprocess(data['Tweet'])\n",
    "\n",
    "textsBJP = data1['Tweet']#.tolist()\n",
    "\n",
    "# Vectorize the preprocessed data\n",
    "vectorized_databjp = hybrid_vectorizer.transform(textsBJP.values.astype('U'))\n",
    "#x_test = vectorizer.transform(x_test.values.astype('U'))\n",
    "\n",
    "afinn_scores1 = Afinn()\n",
    "\n",
    "# Apply AFINN sentiment analysis to the texts\n",
    "afinn_scoresbjp = np.array([afinn_scores1.score(text) for text in textsBJP])\n",
    "afinn_scoresbjp1=afinn_scoresbjp.reshape(-1, 1)\n",
    "\n",
    "# Combine AFINN scores with the vectorized data (if applicable)\n",
    "combined_databjp = hstack((vectorized_databjp, afinn_scoresbjp1))\n",
    "\n",
    "# Make predictions using the trained SVM model\n",
    "predictionsbjp = svm_best.predict(combined_databjp)\n",
    "\n",
    "# Save predictions to a CSV file or use them for further analysis\n",
    "data['predictions'] = predictionsbjp\n",
    "data.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4310c167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9673,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['Tweet'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b88a6f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9673, 20408)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_databjp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21b9e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 1048)\n"
     ]
    }
   ],
   "source": [
    "support_vectors = svm_best.support_vectors_\n",
    "print(support_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933fd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da52bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722f867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78167cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.svm import SVC\n",
    "# from afinn import Afinn\n",
    "# from scipy.sparse import hstack\n",
    "\n",
    "# # Load the prelabeled dataset from a CSV file\n",
    "# df = pd.read_csv('labeled_tweets_dataAL.csv')\n",
    "\n",
    "# # Separate the text and label columns\n",
    "# texts = df['Tweet'].tolist()\n",
    "# labels = df['setiment'].tolist()\n",
    "\n",
    "# # Create an instance of AFINN\n",
    "# afinn = Afinn()\n",
    "\n",
    "# # Apply AFINN sentiment analysis to the texts\n",
    "# afinn_scores = [afinn.score(text) for text in texts]\n",
    "\n",
    "# # Create a document-term matrix using CountVectorizer\n",
    "# #vectorizer = CountVectorizer(stop_words='english')\n",
    "# X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# # Combine AFINN scores with the document-term matrix\n",
    "# X_combined = hstack((X, afinn_scores))\n",
    "\n",
    "# # Split the data into training and validation/test sets\n",
    "# train_texts = X_combined[:120]\n",
    "# train_labels = labels[:120]\n",
    "# val_texts = X_combined[120:]\n",
    "# val_labels = labels[120:]\n",
    "\n",
    "# # Initialize and train the SVM model\n",
    "# svm_model = SVC(C=1.0, kernel='linear')\n",
    "# svm_model.fit(train_texts, train_labels)\n",
    "\n",
    "# # Evaluate the model on the validation/test set\n",
    "# accuracy = svm_model.score(val_texts, val_labels)\n",
    "# print(\"Validation/Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a9c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b6c43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train1, y_test = train_test_split(manually_labeled_data['Tweet'], manually_labeled_data['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3400378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_tweets = vectorizerPrelabeled.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6aa4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# afinn = Afinn()\n",
    "# afinn_scores = np.array([afinn.score(tweet) for tweet in X_train])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1cf97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_features = hstack((vectorized_tweets, afinn_scores.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79d500d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_modelPrelabeled.fit(combined_features, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fd4998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_tweets_test = vectorizerPrelabeled.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11dd6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# afinn_scores_test = np.array([afinn.score(tweet) for tweet in X_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d02607d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_features_test = hstack((vectorized_tweets_test, afinn_scores_test.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9a78fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels = svm_modelPrelabeled.predict(combined_features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "945e0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = (predicted_labels == y_test).mean()\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1b3aa",
   "metadata": {},
   "source": [
    "## Hybrid Approch (using Afinn and svm model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b797e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "af357112",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 918 features, but SVC is expecting 917 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [198]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame(vectorized_tweets\u001b[38;5;241m.\u001b[39mtoarray()), pd\u001b[38;5;241m.\u001b[39mDataFrame(afinn_scores)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the trained SVM model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#svm_model = pickle.load(open('tune_svm_model.pkl', 'rb'))\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Predict the sentiment labels of the combined feature matrix using the SVM model\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m predicted_sentiments \u001b[38;5;241m=\u001b[39m \u001b[43mtune_svm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Add the predicted sentiment labels to the unlabeled data\u001b[39;00m\n\u001b[0;32m     32\u001b[0m unlabeled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentimenthybrid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_sentiments\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:791\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    789\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 791\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:414\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:592\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    589\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 592\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[0;32m    602\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 918 features, but SVC is expecting 917 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the AFINN lexicon\n",
    "afinn = Afinn()\n",
    "\n",
    "# Load the CSV file with unlabeled tweet data\n",
    "unlabeled_data = pd.read_csv('BJPsentiment.csv')\n",
    "\n",
    "# Preprocess the tweet data (assuming it's already preprocessed)\n",
    "\n",
    "# Calculate the AFINN sentiment scores for the preprocessed tweets\n",
    "afinn_scores = [afinn.score(tweet) for tweet in unlabeled_data['Tweet']]\n",
    "\n",
    "# Vectorize the preprocessed tweets using TF-IDF\n",
    "#vectorizer = TfidfVectorizer()\n",
    "vectorized_tweets = tune_svm_vectorizer.transform(unlabeled_data['Tweet'])    ##using the same  tune_svm_vectorizer that was used in fine tune the svm model\n",
    "\n",
    "#vectorized_tweets = vectorizer.transform(unlabeled_data['Tweet'])\n",
    "\n",
    "# Concatenate the AFINN scores with the vectorized tweets\n",
    "combined_features = pd.concat([pd.DataFrame(vectorized_tweets.toarray()), pd.DataFrame(afinn_scores)], axis=1)\n",
    "\n",
    "# Load the trained SVM model\n",
    "#svm_model = pickle.load(open('tune_svm_model.pkl', 'rb'))\n",
    "\n",
    "# Predict the sentiment labels of the combined feature matrix using the SVM model\n",
    "predicted_sentiments = tune_svm_model.predict(combined_features)\n",
    "\n",
    "# Add the predicted sentiment labels to the unlabeled data\n",
    "unlabeled_data['Sentimenthybrid'] = predicted_sentiments\n",
    "\n",
    "# Save the updated unlabeled data with predicted sentiment labels\n",
    "unlabeled_data.to_csv('BJPSentiment.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ceadc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cf051da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9673, 917)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8712f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7aec3ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of features for the SVM model: 917\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected number of features for the SVM model:\", X_train_vectorized.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ce530b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the combined feature matrix: 918\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features in the combined feature matrix:\", combined_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b9da312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "            ...\n",
      "            908, 909, 910, 911, 912, 913, 914, 915, 916,   0],\n",
      "           dtype='int64', length=918)\n"
     ]
    }
   ],
   "source": [
    "print(combined_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "21833e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "9668    0.0\n",
      "9669    0.0\n",
      "9670    0.0\n",
      "9671    0.0\n",
      "9672    0.0\n",
      "Name: 1, Length: 9673, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "column_1 = combined_features.iloc[:, 1]\n",
    "print(column_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6fa5e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9673, 918)\n"
     ]
    }
   ],
   "source": [
    "print(combined_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cd99ff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4    5    6    7    8    9    ...  908  909  910  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "9668  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "9669  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "9670  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "9671  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "9672  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "      911  912  913  914  915  916   0    \n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  11.0  \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0   4.0  \n",
      "...   ...  ...  ...  ...  ...  ...   ...  \n",
      "9668  0.0  0.0  0.0  0.0  0.0  0.0  -3.0  \n",
      "9669  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
      "9670  0.0  0.0  0.0  0.0  0.0  0.0  -2.0  \n",
      "9671  0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
      "9672  0.0  0.0  0.0  0.0  0.0  0.0  13.0  \n",
      "\n",
      "[9673 rows x 918 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d7f44423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119, 917)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "22f576a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9673\n"
     ]
    }
   ],
   "source": [
    "print(len(afinn_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "264b9014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9673, 917)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_tweets.shape)\n",
    "#print(afinn_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7b2de9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [175]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tune_svm_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names()))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:88\u001b[0m, in \u001b[0;36mdeprecated._decorate_fun.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fun)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1429\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_feature_names is deprecated in 1.0 and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 1.2. Please use get_feature_names_out instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1420\u001b[0m )\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_names\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;124;03m\"\"\"Array mapping from feature integer indices to feature name.\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \n\u001b[0;32m   1424\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;124;03m        A list of feature names.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:498\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[1;32m--> 498\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "print(len(tune_svm_vectorizer.get_feature_names()))\n",
    "print(len(vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335d1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f8001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435a3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To evaluate the performance of the tuned SVM model on other evaluation metrics such as precision, recall, F1 score, \n",
    "#or AUC, you can use the test data and calculate these metrics using scikit-learn library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7a7c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# # Assuming you have the tuned SVM model stored in 'tuned_svm_model'\n",
    "# # Assuming you have the test data and labels stored in 'X_test' and 'y_test'\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# # Encode any string labels in y_train and y_test to numeric values\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# y_pred = svm_model.predict(X_test_transformed)\n",
    "# y_pred_encoded = label_encoder.transform(y_pred)\n",
    "\n",
    "# # Calculate evaluation metrics\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='macro')\n",
    "# recall = recall_score(y_test, y_pred, average='macro')\n",
    "# f1 = f1_score(y_test, y_pred, average='macro')\n",
    "# auc = roc_auc_score(y_test_encoded, y_pred_encoded, multi_class='ovr')\n",
    "\n",
    "# # Print the evaluation metrics\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1 Score:\", f1)\n",
    "# print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1904cdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# # Assuming you have the tuned SVM model stored in 'tuned_svm_model'\n",
    "# # Assuming you have the test data and labels stored in 'X_test' and 'y_test'\n",
    "\n",
    "# # Preprocess the test data (apply the same preprocessing steps as the training data)\n",
    "\n",
    "# # Assuming your test data is stored in a list called 'test_documents'\n",
    "# preprocessed_test_documents = preprocess_documents(test_documents)\n",
    "\n",
    "# # Transform the preprocessed test data\n",
    "# X_test_transformed = vectorizer.transform(preprocessed_test_documents)  # Use the same vectorizer as used on the training data\n",
    "\n",
    "# # Make predictions on the transformed test data\n",
    "# y_pred = svm_model.predict(X_test_transformed)\n",
    "\n",
    "# # Calculate evaluation metrics\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# # Print the evaluation metrics\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1 Score:\", f1)\n",
    "# print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd781dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b497cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4e346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf14aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d026e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##rough:handeling errors of above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "696d4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN count: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Count NaN values in the DataFrame\n",
    "nan_count = df.isnull().sum().sum()\n",
    "\n",
    "# Print the total number of NaN values\n",
    "print(f\"Total NaN count: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95940537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As below you can see there is a mismatch in data of X_train_vectorized,and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5bce5a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21978, 24298)\n",
      "(21984,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorized.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab72691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so here to equalizize the data we have trim the last value of y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "89eda179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21978, 24298)\n",
      "(21979,)\n",
      "(21978, 24298)\n",
      "(21978,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of X_train_vectorized and y_train\n",
    "print(X_train_vectorized.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Remove the extra sample from y_train\n",
    "y_train = y_train[:-1]\n",
    "\n",
    "# Check the shapes again\n",
    "print(X_train_vectorized.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6c7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e085d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f66fede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5486,)\n",
      "(5497,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64f9cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5486,)\n",
      "(5487,)\n",
      "(5486,)\n",
      "(5486,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of X_train_vectorized and y_train\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Remove the extra sample from y_train\n",
    "y_test = y_test[:-1]\n",
    "\n",
    "# Check the shapes again\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e326d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so here to equalizize the data we have trim the last value of y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df4ce893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21930, 22461)\n",
      "(21934,)\n",
      "(21930, 22461)\n",
      "(21930,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of X_train_vectorized and y_train\n",
    "print(X_train_vectorized.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Remove the extra sample from y_train\n",
    "y_train = y_train[:-4]\n",
    "\n",
    "# Check the shapes again\n",
    "print(X_train_vectorized.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e539c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6e359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
